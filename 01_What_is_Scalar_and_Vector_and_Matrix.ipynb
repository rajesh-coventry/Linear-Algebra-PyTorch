{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522b112b",
   "metadata": {},
   "source": [
    "# **Scalar, Vector and Matrix:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c7f53",
   "metadata": {},
   "source": [
    "> ![](https://th.bing.com/th/id/R.9948aa39a95c784dd7283c337d0d95c1?rik=m6fMuTEoPirB6w&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62d661",
   "metadata": {},
   "source": [
    "## **1. Scalar:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b95690",
   "metadata": {},
   "source": [
    "A scalar is a single numerical value - essentially just a regular number. It's the most basic mathematical object, representing a quantity with magnitude but no direction, unlike vectors which have both magnitude and direction, or matrices which are arrays of numbers.\n",
    "\n",
    "Scalars are typically denoted using lowercase letters, often in italics:\n",
    "   \n",
    "   - *a*, *b*, *c*, *x*, *y*, *z*\n",
    "   \n",
    "   - Greek letters: *$Î±$* (alpha), *$Î²$* (beta), *$Î»$* (lambda)\n",
    "   \n",
    "   - Sometimes with subscripts: *$aâ‚$*, *$aâ‚‚$*, *$xâ‚€$*\n",
    "\n",
    "In mathematical contexts, scalars are usually written in italics to distinguish them from vectors (bold lowercase) and matrices (bold uppercase).\n",
    "\n",
    "### **Usefulness in Deep Learning:**\n",
    "\n",
    "**Scalars play crucial roles in deep learning:**\n",
    "\n",
    "1. **`Learning Rate`**: Perhaps the most important scalar hyperparameter, controlling how much model parameters are updated during training. A learning rate of 0.001 means parameters are adjusted by small increments.\n",
    "\n",
    "2. **`Loss Values`**: The output of loss functions are scalars representing how well the model is performing. During training, we minimize these scalar values.\n",
    "\n",
    "3. **`Regularization Parameters`**: Lambda values in $L1/L2$ regularization are scalars that control the strength of regularization applied to prevent overfitting.\n",
    "\n",
    "4. **`Batch Size and Epochs`**: These are scalar hyperparameters that determine training behavior - how many samples to process at once and how many times to iterate through the dataset.\n",
    "\n",
    "5. **`Activation Function Parameters`**: Some activation functions use scalar parameters, like the alpha parameter in LeakyReLU or the beta parameter in Swish.\n",
    "\n",
    "6. **`Temperature in Softmax`**: A scalar parameter that controls the \"sharpness\" of probability distributions in classification tasks.\n",
    "\n",
    "### **Defining Scalars in PyTorch:**\n",
    "\n",
    "**PyTorch provides several ways to create and work with scalars:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar-1: 3.140000104904175\n",
      "Scalar-2: 42\n",
      "Scalar-3: 2.0\n",
      "Learning Rate: 0.009999999776482582\n",
      "Loss: 0.5\n",
      "Scaled Loss: 1.0\n",
      "Python umber: 3.14\n",
      "Tensor Scalar: 3.140000104904175\n",
      "Scalaed Value: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating scalars\n",
    "scalar1 = torch.tensor(3.14)           # Float scalar\n",
    "scalar2 = torch.tensor(42)             # Integer scalar\n",
    "scalar3 = torch.tensor(2.0, dtype=torch.float32)  # Explicit dtype\n",
    "print(f\"Scalar-1: {scalar1}\")\n",
    "print(f\"Scalar-2: {scalar2}\")\n",
    "print(f\"Scalar-3: {scalar3}\")\n",
    "# Creating scalars on GPU\n",
    "# scalar_gpu = torch.tensor(5.0, device='cuda')\n",
    "\n",
    "# Creating scalars that require gradients (for optimization)\n",
    "learning_rate = torch.tensor(0.01, requires_grad=True)\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "\n",
    "# Using scalar operations\n",
    "loss = torch.tensor(0.5, requires_grad=True)\n",
    "print(f\"Loss: {loss}\")\n",
    "scaled_loss = loss * 2.0  # Scalar multiplication\n",
    "print(f\"Scaled Loss: {scaled_loss}\")\n",
    "\n",
    "# Converting Python numbers to tensors\n",
    "python_num = 3.14\n",
    "print(f\"Python umber: {python_num}\")\n",
    "tensor_scalar = torch.tensor(python_num)\n",
    "print(f\"Tensor Scalar: {tensor_scalar}\")\n",
    "# Extracting scalar values\n",
    "scalar_value = tensor_scalar.item()  # Returns Python number\n",
    "print(f\"Scalaed Value: {scaled_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572d597",
   "metadata": {},
   "source": [
    "### **Properties of Scalars:**\n",
    "\n",
    "1. **`Commutativity`**:    \n",
    "   \n",
    "   Scalar operations are commutative, meaning *$a + b = b + a$* and *$a Ã— b = b Ã— a$*. This property doesn't hold for matrix operations.\n",
    "\n",
    "2. **`Associativity`**: \n",
    "\n",
    "   For scalars, *$(a + b) + c = a + (b + c)$* and *$(a Ã— b) Ã— c = a Ã— (b Ã— c)$*. This allows flexible grouping in calculations.\n",
    "\n",
    "3. **`Distributivity`**:   \n",
    "   \n",
    "   Scalars distribute over addition: *$a Ã— (b + c) = a Ã— b + a Ã— c$*. This property is fundamental in expanding algebraic expressions.\n",
    "\n",
    "4. **`Identity Elements`**:    \n",
    "   \n",
    "   The additive identity is 0 (*$a + 0 = a$*) and the multiplicative identity is 1 (*$a Ã— 1 = a$*). These are crucial for maintaining values during operations.\n",
    "\n",
    "5. **`Inverse Elements`**:    \n",
    "\n",
    "   Every scalar *$a$* has an additive inverse *$-a$* such that *$a + (-a) = 0$*, and every non-zero scalar has a multiplicative inverse *$1/a$* such that *$a Ã— (1/a) = 1$*.\n",
    "\n",
    "6. **`Scalar Multiplication with Vectors`**:   \n",
    "   \n",
    "   When multiplying a scalar with a vector, the scalar multiplies each component of the vector: *$c Ã— [x, y, z] = [cÃ—x, cÃ—y, cÃ—z]$*. This scales the vector's magnitude while preserving its direction.\n",
    "\n",
    "7. **`Scalar Multiplication with Matrices`**:    \n",
    "   \n",
    "   Similarly, scalar multiplication with matrices multiplies each element: *$c Ã— [[a, b], [c, d]] = [[cÃ—a, cÃ—b], [cÃ—c, cÃ—d]]$*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1cf63e",
   "metadata": {},
   "source": [
    "8.  **`Field Properties`**:    \n",
    "   Scalars typically come from a field (like real numbers $â„$ or complex numbers $â„‚$), which means they satisfy all the algebraic properties mentioned above. This mathematical structure is what makes linear algebra operations well-defined and consistent.\n",
    "\n",
    "9. **`Dimension`**:   \n",
    "    \n",
    "   Scalars are zero-dimensional objects - they have no spatial extent, unlike vectors (1D), matrices (2D), or higher-order tensors.\n",
    "\n",
    "10. **`Invariance`**:   \n",
    "  Scalars remain unchanged under coordinate transformations. While vectors and matrices can change representation when you rotate or translate coordinate systems, scalars maintain their value.\n",
    "\n",
    "These properties make scalars the foundation for more complex linear algebra operations and are essential for understanding how deep learning algorithms manipulate data through mathematical transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183dc42",
   "metadata": {},
   "source": [
    "-------------\n",
    "-------------\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe87a5",
   "metadata": {},
   "source": [
    "## **Number Sets:**\n",
    "\n",
    "\n",
    "**1. Natural Numbers (â„•):**\n",
    "   - **Values**: {1, 2, 3, 4, 5, 6, ...}\n",
    "   - **Description**: Positive counting numbers, sometimes includes 0 depending on context\n",
    "\n",
    "**2. Whole Numbers (ð•Ž):** \n",
    "  - **Values**: {0, 1, 2, 3, 4, 5, ...}\n",
    "  - **Description**: Natural numbers plus zero\n",
    "\n",
    "**3. Integers (â„¤):**\n",
    "  - **Values**: {..., -3, -2, -1, 0, 1, 2, 3, ...}\n",
    "  - **Description**: Whole numbers plus negative numbers\n",
    "\n",
    "**4. Rational Numbers (â„š):**\n",
    "  - **Values**: All fractions p/q where p, q are integers and q â‰  0\n",
    "  - **Examples**: 1/2, -3/4, 5, 0.25, 0.333..., -2.5\n",
    "  - **Description**: Numbers that can be expressed as fractions\n",
    "\n",
    "**5. Irrational Numbers:**\n",
    "  - **Values**: Numbers that cannot be expressed as fractions\n",
    "  - **Examples**: Ï€, e, âˆš2, âˆš3, Ï† (golden ratio)\n",
    "  - **Description**: Non-repeating, non-terminating decimals\n",
    "\n",
    "**6. Real Numbers (â„):**  \n",
    "  - **Values**: All rational and irrational numbers combined\n",
    "  - **Description**: All numbers on the number line, includes everything above\n",
    "\n",
    "**7. Complex Numbers (â„‚):**\n",
    "  - **Values**: Numbers of the form a + bi where a, b are real and i = âˆš(-1)\n",
    "  - **Examples**: 3 + 4i, -2 - 5i, 7 (pure real), 3i (pure imaginary)\n",
    "  - **Description**: Includes real numbers plus imaginary numbers\n",
    "\n",
    "**Quick Memory Aid:**\n",
    "> $â„• âŠ‚ ð•Ž âŠ‚ â„¤ âŠ‚ â„š âŠ‚ â„ âŠ‚ â„‚$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fe1ce",
   "metadata": {},
   "source": [
    "-----\n",
    "-----------\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b8231",
   "metadata": {},
   "source": [
    "## **2. Vectors:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09455842",
   "metadata": {},
   "source": [
    "> ![](https://cdn1.byjus.com/wp-content/uploads/2021/03/Vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82b25c",
   "metadata": {},
   "source": [
    "A vector is an ordered collection of numbers (called `components` or `elements`) that represents a quantity with `both magnitude and direction`. Unlike scalars which are just single numbers, vectors contain multiple values and can represent `points in space`, `directions`, or `collections of related data`.\n",
    "\n",
    "**Representation and Notation:**\n",
    "\n",
    "**Mathematical Notation**:\n",
    "   - **Bold lowercase letters**: **$v$**, **$u$**, **$a$**, **$x$**\n",
    "\n",
    "   - **Arrows**: vâƒ—, uâƒ—, aâƒ—\n",
    "\n",
    "   - **Component form**: $v = [vâ‚, vâ‚‚, vâ‚ƒ]$ or $v = (vâ‚, vâ‚‚, vâ‚ƒ)$\n",
    "   \n",
    "   - **Column vector**: \n",
    "  ```raw\n",
    "        v = [vâ‚]\n",
    "            [vâ‚‚]\n",
    "            [vâ‚ƒ]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021dca5",
   "metadata": {},
   "source": [
    "**Common Representations**:\n",
    "   - **2D vector**: $v = [3, 4]$ represents a point at coordinates $(3, 4)$\n",
    "\n",
    "   - **3D vector**: $v = [1, -2, 5]$ represents a point in $3D$ space\n",
    "\n",
    "   - **n-dimensional**: $v = [vâ‚, vâ‚‚, ..., vâ‚™]$ for high-dimensional spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f871d78",
   "metadata": {},
   "source": [
    "### **Geometric and Linear Algebraic Meanings:**\n",
    "\n",
    "**1. Geometric Interpretation**:  \n",
    "   - Vectors represent arrows in space with specific direction and magnitude. \n",
    "   \n",
    "   - A vector $[3, 4]$ can be visualized as an arrow starting from the origin $(0, 0)$ and pointing to the coordinate $(3, 4)$. \n",
    "   \n",
    "   - The length of this arrow is the magnitude $âˆš(3Â² + 4Â²) = 5$, and it points in a specific direction from the origin.\n",
    "\n",
    "**2. Linear Algebraic Interpretation**:\n",
    "   - Vectors are elements of vector spaces that can be added together and multiplied by scalars. \n",
    "   \n",
    "   - They represent solutions to linear equations, transformations, and serve as building blocks for more complex mathematical structures. \n",
    "   \n",
    "   - In linear algebra, vectors are often viewed as columns or rows of numbers that can be manipulated through matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58a2ed",
   "metadata": {},
   "source": [
    "## **Usefulness in Deep Learning:**\n",
    "\n",
    "1. **`Feature Representation`**:   \n",
    "Each data sample is represented as a vector where each component represents a feature. For example, in image processing, a 28Ã—28 pixel image becomes a 784-dimensional vector where each element represents a pixel intensity.\n",
    "\n",
    "2. **`Word Embeddings`**:   \n",
    "Words are converted into dense vectors that capture semantic meaning. Similar words have similar vector representations, enabling machines to understand relationships between words.\n",
    "\n",
    "3. **`Neural Network Weights`**:   \n",
    "Model parameters are stored as vectors, allowing efficient computation of gradients and parameter updates during training.\n",
    "\n",
    "4. **`Batch Processing`**:   \n",
    "Multiple data samples are processed simultaneously by organizing them into vectors and matrices, dramatically improving computational efficiency.\n",
    "\n",
    "5. **`Gradient Computation`**:   \n",
    "Gradients in neural networks are vectors pointing in the direction of steepest increase of the loss function, enabling optimization algorithms to update parameters effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2884e7",
   "metadata": {},
   "source": [
    "### **Row Vector and Column Vector:** \n",
    "\n",
    "**1. Horizontal Form (Row Vector):**\n",
    "   - Components are arranged horizontally in a single row.\n",
    "\n",
    "   - **Notation**:\n",
    "      - **v** = [3, 4, 5]\n",
    "      - **v** = (3, 4, 5)\n",
    "      - **v** = âŸ¨3, 4, 5âŸ©\n",
    "\n",
    "   - **Matrix Form**: $1Ã—n$ matrix **(1 row, n columns)**\n",
    "\n",
    "   - $v = [3,   4,   5]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd84033",
   "metadata": {},
   "source": [
    "**2. Vertical Form (Column Vector):**  \n",
    "\n",
    "   - Components are arranged vertically in a single column.\n",
    "\n",
    "   - **Notation**:\n",
    "```raw\n",
    "                v = [3]\n",
    "                    [4]\n",
    "                    [5]\n",
    "```\n",
    "\n",
    "   - **Matrix Form**: $nÃ—1$ matrix **(n rows, 1 column)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6868084",
   "metadata": {},
   "source": [
    "**Key Differences:**\n",
    "\n",
    "**1. Matrix Dimensions**:\n",
    "   - **Horizontal (Row)**: Shape is $(1, n)$ - `one row`, `multiple columns`\n",
    "\n",
    "   - **Vertical (Column)**: Shape is $(n, 1)$ - `multiple rows`, `one column`\n",
    "\n",
    "**2. Matrix Multiplication Rules**:\n",
    "```raw\n",
    "    Row vector Ã— Matrix:\n",
    "      [1  2  3] Ã— [a  b] = [1a+2c+3e  1b+2d+3f]\n",
    "                  [c  d]\n",
    "                  [e  f]\n",
    "\n",
    "    Matrix Ã— Column vector:\n",
    "    [a  b] Ã— [1] = [aÃ—1 + bÃ—2]\n",
    "    [c  d]   [2]   [cÃ—1 + dÃ—2]\n",
    "```\n",
    "\n",
    "**3. Transpose Relationship**:\n",
    "  - Row vector = **(Column vector)áµ€**\n",
    "  - Column vector = **(Row vector)áµ€**\n",
    "\n",
    "**4. PyTorch Implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "437395cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row vector: tensor([[3, 4, 5]])\n",
      "Row shape: torch.Size([1, 3])\n",
      "Column vector:\n",
      "tensor([[3],\n",
      "        [4],\n",
      "        [5]])\n",
      "Column shape: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Horizontal form (row vector)\n",
    "row_vector = torch.tensor([[3, 4, 5]])  # Shape: (1, 3)\n",
    "\n",
    "# Vertical form (column vector)  \n",
    "col_vector = torch.tensor([[3], [4], [5]])  # Shape: (3, 1)\n",
    "\n",
    "# Converting between forms\n",
    "row_to_col = row_vector.T  # or row_vector.transpose(0, 1)\n",
    "col_to_row = col_vector.T\n",
    "\n",
    "print(f\"Row vector: {row_vector}\")\n",
    "print(f\"Row shape: {row_vector.shape}\")\n",
    "print(f\"Column vector:\\n{col_vector}\")\n",
    "print(f\"Column shape: {col_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d36bf",
   "metadata": {},
   "source": [
    "**5. Geometric Interpretation**:  \n",
    "Both represent the same point in space or the same direction, just written differently:\n",
    "   - **`Horizontal`**: $[3, 4, 5]$ represents point $(3, 4, 5)$\n",
    "   - **`Vertical`**: Same point $(3, 4, 5)$, just written as a column\n",
    "\n",
    "**6. Usage in Different Contexts**:\n",
    "\n",
    "**Row vectors are common in**:\n",
    "   - Machine learning input data (features arranged horizontally)\n",
    "   - Probability distributions\n",
    "   - When vectors represent observations or samples\n",
    "\n",
    "**Column vectors are common in**:\n",
    "   - Traditional linear algebra textbooks\n",
    "   - System of linear equations $(Ax = b)$\n",
    "   - Geometric transformations\n",
    "   - When vectors represent variables or unknowns\n",
    "\n",
    "**7. Dot Product Calculation**:\n",
    "\n",
    "```raw\n",
    "    Row Ã— Column: [1  2  3] Ã— [4] = 1Ã—4 + 2Ã—5 + 3Ã—6 = 32\n",
    "                              [5]\n",
    "                              [6]\n",
    "\n",
    "    Column Ã— Row: [1] Ã— [4  5  6] = [4   5   6 ]\n",
    "                  [2]               [8   10  12]\n",
    "                  [3]               [12  15  18]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835477b",
   "metadata": {},
   "source": [
    "**8. Memory Layout**:  \n",
    "   - **`Row vectors`**: Elements stored consecutively in memory\n",
    "   - **`Column vectors`**: May have different memory stride patterns in matrices\n",
    "\n",
    "**Important Note**:   \n",
    "In most programming contexts and machine learning frameworks, the distinction matters primarily for matrix operations. For basic vector operations like addition, subtraction, and scalar multiplication, both forms behave identically - it's mainly about how we organize our data for efficient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872f44d",
   "metadata": {},
   "source": [
    "### **PyTorch Implementation Scenarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feee8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Vector: tensor([1, 2, 3, 4, 5])\n",
      "2D Row Vector: tensor([[1, 2, 3]])\n",
      "Column Vector:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n",
      "1D Vector Shape: torch.Size([5])\n",
      "2D Row Vector Shape: torch.Size([1, 3])\n",
      "Column Vector Shape: torch.Size([3, 1])\n",
      "\n",
      "Vector Operations:\n",
      "a + b = tensor([5, 7, 9])\n",
      "a - b = tensor([-3, -3, -3])\n",
      "a Â· b = 32\n",
      "a Ã— b = tensor([-3,  6, -3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. BASIC VECTOR CREATION AND OPERATIONS \n",
    "\n",
    "# Creating vectors\n",
    "vector_1d = torch.tensor([1, 2, 3, 4, 5])  # 1D vector\n",
    "vector_2d = torch.tensor([[1, 2, 3]])      # Row vector (1x3)\n",
    "vector_col = torch.tensor([[1], [2], [3]]) # Column vector (3x1)\n",
    "\n",
    "print(\"1D Vector:\", vector_1d)\n",
    "print(\"2D Row Vector:\", vector_2d)\n",
    "print(\"Column Vector:\\n\", vector_col)\n",
    "print()\n",
    "print(\"1D Vector Shape:\", vector_1d.shape)\n",
    "print(\"2D Row Vector Shape:\", vector_2d.shape)\n",
    "print(\"Column Vector Shape:\", vector_col.shape)\n",
    "\n",
    "# Vector arithmetic\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "addition = a + b           # Element-wise addition\n",
    "subtraction = a - b        # Element-wise subtraction\n",
    "dot_product = torch.dot(a, b)  # Dot product\n",
    "cross_product = torch.cross(a, b)  # Cross product (3D only)\n",
    "\n",
    "print(f\"\\nVector Operations:\")\n",
    "print(f\"a + b = {addition}\")\n",
    "print(f\"a - b = {subtraction}\")\n",
    "print(f\"a Â· b = {dot_product}\")\n",
    "print(f\"a Ã— b = {cross_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d27841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single image features:\n",
      "    tensor([0.8000, 0.6000, 0.3000, 0.7000, 0.4000, 0.9000])\n",
      "Single Image Features Shape: torch.Size([6])\n",
      "\n",
      "Image Batch features:\n",
      "    tensor([[0.8000, 0.6000, 0.3000, 0.7000, 0.4000, 0.9000],\n",
      "        [0.2000, 0.8000, 0.9000, 0.1000, 0.6000, 0.3000],\n",
      "        [0.5000, 0.4000, 0.6000, 0.8000, 0.2000, 0.7000]])\n",
      "Batch shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 2. FEATURE VECTORS FOR MACHINE LEARNING\n",
    "\n",
    "\n",
    "# Example: Image classification features\n",
    "# Simulating features extracted from an image\n",
    "image_features = torch.tensor([\n",
    "    0.8,   # brightness\n",
    "    0.6,   # contrast\n",
    "    0.3,   # red_intensity\n",
    "    0.7,   # green_intensity\n",
    "    0.4,   # blue_intensity\n",
    "    0.9    # sharpness\n",
    "])\n",
    "\n",
    "# Batch of feature vectors (multiple samples)\n",
    "batch_features = torch.tensor([\n",
    "    [0.8, 0.6, 0.3, 0.7, 0.4, 0.9],  # Image 1\n",
    "    [0.2, 0.8, 0.9, 0.1, 0.6, 0.3],  # Image 2\n",
    "    [0.5, 0.4, 0.6, 0.8, 0.2, 0.7],  # Image 3\n",
    "])\n",
    "\n",
    "print(f\"\\nSingle image features:\\n    {image_features}\")\n",
    "print(f\"Single Image Features Shape: {image_features.shape}\")\n",
    "print()\n",
    "print(f\"Image Batch features:\\n    {batch_features}\")\n",
    "print(f\"Batch shape: {batch_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ea292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embeddings shape: torch.Size([4, 100])\n",
      "Cosine similarity between word 1 and word 2: -0.1919\n"
     ]
    }
   ],
   "source": [
    "# 3. WORD EMBEDDINGS\n",
    "\n",
    "# Creating word embeddings\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Sample word indices\n",
    "word_indices = torch.tensor([1, 15, 30, 45])\n",
    "\n",
    "# Get word vectors\n",
    "word_vectors = embedding(word_indices)\n",
    "print(f\"\\nWord embeddings shape: {word_vectors.shape}\")\n",
    "\n",
    "# Similarity between words using cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n",
    "\n",
    "similarity = cosine_similarity(word_vectors[0], word_vectors[1])\n",
    "print(f\"Cosine similarity between word 1 and word 2: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73873176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient w.r.t input: tensor([[-2.7947, -0.9862, -2.3241]])\n",
      "Weight gradients: tensor([[ -5.8407, -11.6814, -17.5221]])\n",
      "Bias gradient: tensor([-5.8407])\n"
     ]
    }
   ],
   "source": [
    "# 4. GRADIENT VECTORS\n",
    "\n",
    "# Simple neural network to demonstrate gradient vectors\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "y_true = torch.tensor([[5.0]])\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "loss = F.mse_loss(y_pred, y_true)\n",
    "\n",
    "# Backward pass - compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Access gradient vectors\n",
    "print(f\"\\nGradient w.r.t input: {x.grad}\")\n",
    "print(f\"Weight gradients: {model.linear.weight.grad}\")\n",
    "print(f\"Bias gradient: {model.linear.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b919099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vector: tensor([1., 2.])\n",
      "Transformed vector: tensor([ 5., 11.])\n"
     ]
    }
   ],
   "source": [
    "# 5. VECTOR TRANSFORMATIONS\n",
    "\n",
    "\n",
    "# Linear transformation using matrix multiplication\n",
    "transform_matrix = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "input_vector = torch.tensor([1, 2], dtype=torch.float32)\n",
    "\n",
    "# Apply transformation\n",
    "transformed = torch.mv(transform_matrix, input_vector)\n",
    "print(f\"\\nOriginal vector: {input_vector}\")\n",
    "print(f\"Transformed vector: {transformed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344572fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention mechanism:\n",
      "Query shape: torch.Size([5, 4])\n",
      "Attention weights shape: torch.Size([5, 5])\n",
      "Attended values shape: torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# 6. ATTENTION MECHANISMS (QUERY, KEY, VALUE VECTORS)\n",
    "\n",
    "\n",
    "# Simplified attention mechanism\n",
    "seq_length = 5\n",
    "d_model = 4\n",
    "\n",
    "# Create query, key, value vectors\n",
    "queries = torch.randn(seq_length, d_model)\n",
    "keys = torch.randn(seq_length, d_model)\n",
    "values = torch.randn(seq_length, d_model)\n",
    "\n",
    "# Compute attention scores\n",
    "attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# Apply attention\n",
    "attended_values = torch.matmul(attention_weights, values)\n",
    "\n",
    "print(f\"\\nAttention mechanism:\")\n",
    "print(f\"Query shape: {queries.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Attended values shape: {attended_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016e154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction probabilities:\n",
      "tensor([[0.5455, 0.0822, 0.3723],\n",
      "        [0.2699, 0.5948, 0.1352],\n",
      "        [0.6584, 0.0803, 0.2613],\n",
      "        [0.2992, 0.3410, 0.3597]])\n",
      "Cross-entropy loss: 0.9185\n"
     ]
    }
   ],
   "source": [
    "# 7. LOSS FUNCTION VECTORS\n",
    "\n",
    "\n",
    "# Multi-class classification scenario\n",
    "num_classes = 3\n",
    "batch_size = 4\n",
    "\n",
    "# Model predictions (logits)\n",
    "logits = torch.randn(batch_size, num_classes)\n",
    "# True labels\n",
    "targets = torch.tensor([0, 1, 2, 0])\n",
    "\n",
    "# Convert to probability vectors\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "print(f\"\\nPrediction probabilities:\\n{probabilities}\")\n",
    "\n",
    "# Cross-entropy loss\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(f\"Cross-entropy loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d1db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector norms and distances:\n",
      "L1 norm of v1: 7.0\n",
      "L2 norm of v1: 5.0\n",
      "Max norm of v1: 4.0\n",
      "Euclidean distance: 2.8284270763397217\n",
      "Manhattan distance: 4.0\n"
     ]
    }
   ],
   "source": [
    "# 8. VECTOR NORMS AND DISTANCES\n",
    "\n",
    "\n",
    "v1 = torch.tensor([3.0, 4.0])\n",
    "v2 = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Different norms\n",
    "l1_norm = torch.norm(v1, p=1)      # L1 norm (Manhattan distance)\n",
    "l2_norm = torch.norm(v1, p=2)      # L2 norm (Euclidean distance)\n",
    "max_norm = torch.norm(v1, p=float('inf'))  # Max norm\n",
    "\n",
    "# Distance between vectors\n",
    "euclidean_distance = torch.norm(v1 - v2)\n",
    "manhattan_distance = torch.norm(v1 - v2, p=1)\n",
    "\n",
    "print(f\"\\nVector norms and distances:\")\n",
    "print(f\"L1 norm of v1: {l1_norm}\")\n",
    "print(f\"L2 norm of v1: {l2_norm}\")\n",
    "print(f\"Max norm of v1: {max_norm}\")\n",
    "print(f\"Euclidean distance: {euclidean_distance}\")\n",
    "print(f\"Manhattan distance: {manhattan_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aae475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vector: tensor([3., 4.])\n",
      "Unit vector: tensor([0.6000, 0.8000])\n",
      "Unit vector norm: 1.0\n",
      "\n",
      "Batch vectors shape: torch.Size([10, 5])\n",
      "Normalized batch shape: torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# 9. VECTOR NORMALIZATION\n",
    "\n",
    "\n",
    "# Unit vector (normalized)\n",
    "unit_vector = v1 / torch.norm(v1)\n",
    "print(f\"\\nOriginal vector: {v1}\")\n",
    "print(f\"Unit vector: {unit_vector}\")\n",
    "print(f\"Unit vector norm: {torch.norm(unit_vector)}\")\n",
    "\n",
    "# Batch normalization on vectors\n",
    "batch_vectors = torch.randn(10, 5)  # 10 vectors of dimension 5\n",
    "normalized_batch = F.normalize(batch_vectors, p=2, dim=1)\n",
    "print(f\"\\nBatch vectors shape: {batch_vectors.shape}\")\n",
    "print(f\"Normalized batch shape: {normalized_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f1629ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenated vector: tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Stacked vectors shape: torch.Size([3, 3])\n",
      "Reshaped to matrix:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# 10. VECTOR CONCATENATION AND RESHAPING\n",
    "\n",
    "\n",
    "# Concatenating vectors\n",
    "v_a = torch.tensor([1, 2, 3])\n",
    "v_b = torch.tensor([4, 5, 6])\n",
    "v_c = torch.tensor([7, 8, 9])\n",
    "\n",
    "# Concatenate along dimension 0\n",
    "concatenated = torch.cat([v_a, v_b, v_c], dim=0)\n",
    "print(f\"\\nConcatenated vector: {concatenated}\")\n",
    "\n",
    "# Stack vectors (creates new dimension)\n",
    "stacked = torch.stack([v_a, v_b, v_c], dim=0)\n",
    "print(f\"Stacked vectors shape: {stacked.shape}\")\n",
    "\n",
    "# Reshape vector\n",
    "reshaped = concatenated.view(3, 3)\n",
    "print(f\"Reshaped to matrix:\\n{reshaped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29276472",
   "metadata": {},
   "source": [
    "### **Properties of Vectors:**\n",
    "\n",
    "**1. Addition and Subtraction**:     \n",
    "  Vectors can be added or subtracted element-wise. For vectors **$a$** = $[1, 2, 3]$ and **$b$** = $[4, 5, 6]$, their sum is **$a$** + **$b$** = $[5, 7, 9]$. This operation is commutative (**$a$** + **$b$** = **$b$** + **$a$**) and associative ((**$a$** + **$b$**) + **$c$** = **$a$** + (**$b$** + **$c$**)).\n",
    "\n",
    "**2. Scalar Multiplication**:   \n",
    "Multiplying a vector by a scalar scales all components uniformly. If **$v$** = $[2, 3, 4]$ and scalar $c = 3$, then $c$**$v$** = $[6, 9, 12]$. This operation preserves the direction of the vector but changes its magnitude.\n",
    "\n",
    "**3. Dot Product (Inner Product)**:   \n",
    "The dot product of two vectors produces a scalar value. For **$a$** = $[1, 2, 3]$ and **$b$** = $[4, 5, 6]$, the dot product is **$a$** Â· **$b$** = $1Ã—4 + 2Ã—5 + 3Ã—6 = 32$. This operation is commutative and measures the similarity between vectors.\n",
    "\n",
    "**4. Cross Product**:   \n",
    "In 3D space, the cross product of two vectors produces a third vector perpendicular to both original vectors. For **$a$** = $[1, 2, 3]$ and **$b$** = $[4, 5, 6]$, **$a$** Ã— **$b$** = $[2Ã—6 - 3Ã—5, 3Ã—4 - 1Ã—6, 1Ã—5 - 2Ã—4] = [-3, 6, -3]$.\n",
    "\n",
    "**5. Magnitude (Norm)**:   \n",
    "The magnitude of a vector represents its length. For vector **$v$** = $[3, 4]$, the magnitude is ||**$v$**|| = $âˆš(3Â² + 4Â²) = 5$. Different norms exist: `L1 norm (Manhattan distance)`, `L2 norm (Euclidean distance)`, and `infinity norm (maximum component)`.\n",
    "\n",
    "**6. Unit Vectors**:   \n",
    "A unit vector has magnitude 1 and represents pure direction. Any vector can be normalized to create a unit vector by dividing by its magnitude: **$Ã»$** = **$v$**/||**$v$**||.\n",
    "\n",
    "**7. Linear Independence**:   \n",
    "A set of vectors is linearly independent if none can be expressed as a linear combination of the others. This property is crucial for determining the dimension of vector spaces and the uniqueness of solutions to linear systems.\n",
    "\n",
    "**8. Orthogonality**:   \n",
    "Two vectors are orthogonal if their dot product is zero, meaning they're perpendicular. Orthogonal vectors are particularly useful in machine learning for creating uncorrelated features and in optimization algorithms.\n",
    "\n",
    "**9. Span**:   \n",
    "The span of a set of vectors is the collection of all possible linear combinations of those vectors. It represents the space that can be reached by combining the vectors with different scalar weights.\n",
    "\n",
    "**10. Dimensionality**:   \n",
    "The number of components in a vector determines its dimensionality. A 3D vector has three components, while high-dimensional vectors in machine learning can have thousands or millions of components.\n",
    "\n",
    "**11. Basis Vectors**:   \n",
    "A basis is a set of linearly independent vectors that span the entire vector space. Any vector in the space can be uniquely expressed as a linear combination of basis vectors.\n",
    "\n",
    "**12. Distance and Similarity**:   \n",
    "Vectors can be compared using various distance metrics. Euclidean distance measures straight-line distance, while cosine similarity measures the angle between vectors, making it useful for comparing directions regardless of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f92a56",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ff50b",
   "metadata": {},
   "source": [
    "## **3. Matrix:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b020c",
   "metadata": {},
   "source": [
    "> ![](https://image.slidesharecdn.com/matrix-130924233933-phpapp01/95/linear-algebra-and-matrix-3-638.jpg?cb=1380066049)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ea4b7",
   "metadata": {},
   "source": [
    "> ![](https://th.bing.com/th/id/R.efe53963822fb24b15835f0b93f3aaa3?rik=jDrYL3UYAYtd6A&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e5151",
   "metadata": {},
   "source": [
    "In linear algebra, a **`matrix`** is a rectangular array or grid of numbers, symbols, or expressions arranged in rows and columns. It's a fundamental concept used to represent and manipulate data in a structured way.\n",
    "\n",
    "Matrices are typically denoted by a bold capital letter, such as **$A$**. The elements within the matrix are represented by lowercase letters with subscripts indicating their position. For an element in the *i*-th row and *j*-th column of a matrix **$A$**, the notation is $a\\_{ij}$.\n",
    "\n",
    "An $m\\times n$ matrix has *m* rows and *n* columns. For example, a 2x3 matrix **A** is written as:\n",
    "\n",
    "$A = \\begin{pmatrix} \n",
    "a_{11} & a_{12} & a_{13} \\\\ \n",
    "a_{21} & a_{22} & a_{23} \n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94375c",
   "metadata": {},
   "source": [
    "### **Matrices in Deep Learning ðŸ§ :**\n",
    "\n",
    "Matrices are at the heart of deep learning because they provide a concise and efficient way to represent and process the vast amounts of data that neural networks handle.\n",
    "\n",
    "At its core, a neural network is a series of mathematical operations, and matrices are the perfect tool for performing these computations on a large scale. Hereâ€™s a breakdown of how matrices are integral to deep learning:\n",
    "\n",
    "  * **`Data Representation`**:   \n",
    "  Datasets, whether they consist of images, text, or tabular data, are converted into matrices. For instance, a grayscale image can be represented as a matrix where each element corresponds to the pixel intensity. A batch of images is then represented as a higher-dimensional matrix.\n",
    "\n",
    "  * **`Model Parameters`**:   \n",
    "  The `\"knowledge\"` a neural network learns is stored in its parameters, which are organized into `matrices`. The connections between neurons in different layers of a neural network are represented by a `weight matrix`. As the network trains, these weights are adjusted through a process called backpropagation, which heavily relies on `matrix calculus`.\n",
    "\n",
    "  * **`Linear Transformations`**:   \n",
    "  Each layer in a neural network applies a `linear transformation` to the data it receives from the previous layer. This transformation is a `matrix-vector` or `matrix-matrix` multiplication. For a given layer, the input data (represented as a `vector` or `matrix`) is multiplied by the layer's `weight matrix`.\n",
    "\n",
    "  * **`Computational Efficiency`**:   \n",
    "  Modern computing hardware, especially Graphics Processing Units $(GPUs)$ and Tensor Processing Units $(TPUs)$, are highly optimized for matrix operations. Performing calculations on large matrices is significantly faster than iterating through individual data points, which is crucial for training deep learning models on massive datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5b2cd",
   "metadata": {},
   "source": [
    "## **Forming Matrices in NumPy and PyTorch:**\n",
    "\n",
    "1. NumPy is a fundamental package for scientific computing in Python. Creating a matrix is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a39adcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 3x3 matrix from a list of lists\n",
    "numpy_matrix = np.array([[1, 2, 3],\n",
    "                         [4, 5, 6],\n",
    "                         [7, 8, 9]])\n",
    "\n",
    "print(numpy_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9cadb",
   "metadata": {},
   "source": [
    "2. PyTorch is a popular open-source machine learning library. Its core data structure is the tensor, which can be used to represent matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf727ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 3x3 matrix (tensor)\n",
    "pytorch_matrix = torch.tensor([[1, 2, 3],\n",
    "                               [4, 5, 6],\n",
    "                               [7, 8, 9]])\n",
    "\n",
    "print(pytorch_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66eaf0b",
   "metadata": {},
   "source": [
    "### **Matrix vs. Array vs. Tensor: What's the Difference?**\n",
    "\n",
    "While these terms are often used interchangeably in programming contexts, they have distinct mathematical and conceptual meanings.\n",
    "\n",
    "| Term    | Description                                                                                                                               |\n",
    "| :------ | :---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Array** | In programming, an **array** is a data structure that stores a collection of elements. It can be one-dimensional, two-dimensional (like a matrix), or multi-dimensional. It's a more general concept. |\n",
    "| **Matrix** | In linear algebra, a **matrix** is a two-dimensional array of numbers that has specific mathematical properties and operations defined on it, such as matrix multiplication and inversion. |\n",
    "| **Tensor** | A **tensor** is a generalization of scalars, vectors, and matrices to higher dimensions. A scalar is a rank-0 tensor, a vector is a rank-1 tensor, and a matrix is a rank-2 tensor. Tensors are the fundamental data structure in deep learning frameworks like PyTorch and TensorFlow. |\n",
    "\n",
    "A `matrix is a specific type of array` with a rich set of mathematical rules. A `tensor is a further generalization that can encompass data of any dimensionality`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054ff90",
   "metadata": {},
   "source": [
    "### **Properties of Matrices in Linear Algebra:**\n",
    "\n",
    "Matrices have several important properties that govern their behavior in mathematical operations. For matrices **$A$**, **$B$**, and **4** of the same size, and a scalar *$k$*:\n",
    "\n",
    "#### 1. **Addition:**\n",
    "\n",
    "  * **Commutative Property**: $A + B = B + A$\n",
    "  * **Associative Property**: $(A + B) + C = A + (B + C)$\n",
    "  * **Additive Identity**: There exists a zero matrix **$0$** such that $A + 0 = A$.\n",
    "  * **Additive Inverse**: For every matrix **$A$**, there exists a matrix **$-A$** such that $A + (-A) = 0$.\n",
    "\n",
    "#### 2. **Scalar Multiplication:**\n",
    "\n",
    "  * **Distributive Property**: $k(A + B) = kA + kB$\n",
    "  * **Distributive Property**: $(k + l)A = kA + lA$ (where *$l$* is also a scalar)\n",
    "  * **Associative Property**: $(kl)A = k(lA)$\n",
    "\n",
    "#### 3. **Matrix Multiplication:**\n",
    "\n",
    "  * **Associative Property**: $(AB)C = A(BC)$\n",
    "  * **Distributive Property**: $A(B + C) = AB + AC$ and $(A + B)C = AC + BC$\n",
    "  * **Multiplicative Identity**: For a square matrix **A**, there exists an identity matrix **I** of the same size such that $AI = IA = A$.\n",
    "  * **Not Commutative**: In general, $AB \\neq BA$. The order of multiplication matters.\n",
    "\n",
    "#### 4. **Transpose:**\n",
    "\n",
    "The transpose of a matrix **A**, denoted as $A^T$, is obtained by swapping its rows and columns.\n",
    "\n",
    "  * **Transpose of a Transpose**: $(A^T)^T = A$\n",
    "  * **Transpose of a Sum**: $(A + B)^T = A^T + B^T$\n",
    "  * **Transpose of a Product**: $(AB)^T = B^T A^T$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
