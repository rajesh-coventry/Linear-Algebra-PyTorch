{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522b112b",
   "metadata": {},
   "source": [
    "# **Scalar, Vector and Matrix:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c7f53",
   "metadata": {},
   "source": [
    "> ![](https://th.bing.com/th/id/R.9948aa39a95c784dd7283c337d0d95c1?rik=m6fMuTEoPirB6w&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62d661",
   "metadata": {},
   "source": [
    "## **1. Scalar:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b95690",
   "metadata": {},
   "source": [
    "A scalar is a single numerical value - essentially just a regular number. It's the most basic mathematical object, representing a quantity with magnitude but no direction, unlike vectors which have both magnitude and direction, or matrices which are arrays of numbers.\n",
    "\n",
    "Scalars are typically denoted using lowercase letters, often in italics:\n",
    "   \n",
    "   - *a*, *b*, *c*, *x*, *y*, *z*\n",
    "   \n",
    "   - Greek letters: *$α$* (alpha), *$β$* (beta), *$λ$* (lambda)\n",
    "   \n",
    "   - Sometimes with subscripts: *$a₁$*, *$a₂$*, *$x₀$*\n",
    "\n",
    "In mathematical contexts, scalars are usually written in italics to distinguish them from vectors (bold lowercase) and matrices (bold uppercase).\n",
    "\n",
    "### **Usefulness in Deep Learning:**\n",
    "\n",
    "**Scalars play crucial roles in deep learning:**\n",
    "\n",
    "1. **`Learning Rate`**: Perhaps the most important scalar hyperparameter, controlling how much model parameters are updated during training. A learning rate of 0.001 means parameters are adjusted by small increments.\n",
    "\n",
    "2. **`Loss Values`**: The output of loss functions are scalars representing how well the model is performing. During training, we minimize these scalar values.\n",
    "\n",
    "3. **`Regularization Parameters`**: Lambda values in $L1/L2$ regularization are scalars that control the strength of regularization applied to prevent overfitting.\n",
    "\n",
    "4. **`Batch Size and Epochs`**: These are scalar hyperparameters that determine training behavior - how many samples to process at once and how many times to iterate through the dataset.\n",
    "\n",
    "5. **`Activation Function Parameters`**: Some activation functions use scalar parameters, like the alpha parameter in LeakyReLU or the beta parameter in Swish.\n",
    "\n",
    "6. **`Temperature in Softmax`**: A scalar parameter that controls the \"sharpness\" of probability distributions in classification tasks.\n",
    "\n",
    "### **Defining Scalars in PyTorch:**\n",
    "\n",
    "**PyTorch provides several ways to create and work with scalars:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar-1: 3.140000104904175\n",
      "Scalar-2: 42\n",
      "Scalar-3: 2.0\n",
      "Learning Rate: 0.009999999776482582\n",
      "Loss: 0.5\n",
      "Scaled Loss: 1.0\n",
      "Python umber: 3.14\n",
      "Tensor Scalar: 3.140000104904175\n",
      "Scalaed Value: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating scalars\n",
    "scalar1 = torch.tensor(3.14)           # Float scalar\n",
    "scalar2 = torch.tensor(42)             # Integer scalar\n",
    "scalar3 = torch.tensor(2.0, dtype=torch.float32)  # Explicit dtype\n",
    "print(f\"Scalar-1: {scalar1}\")\n",
    "print(f\"Scalar-2: {scalar2}\")\n",
    "print(f\"Scalar-3: {scalar3}\")\n",
    "# Creating scalars on GPU\n",
    "# scalar_gpu = torch.tensor(5.0, device='cuda')\n",
    "\n",
    "# Creating scalars that require gradients (for optimization)\n",
    "learning_rate = torch.tensor(0.01, requires_grad=True)\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "\n",
    "# Using scalar operations\n",
    "loss = torch.tensor(0.5, requires_grad=True)\n",
    "print(f\"Loss: {loss}\")\n",
    "scaled_loss = loss * 2.0  # Scalar multiplication\n",
    "print(f\"Scaled Loss: {scaled_loss}\")\n",
    "\n",
    "# Converting Python numbers to tensors\n",
    "python_num = 3.14\n",
    "print(f\"Python umber: {python_num}\")\n",
    "tensor_scalar = torch.tensor(python_num)\n",
    "print(f\"Tensor Scalar: {tensor_scalar}\")\n",
    "# Extracting scalar values\n",
    "scalar_value = tensor_scalar.item()  # Returns Python number\n",
    "print(f\"Scalaed Value: {scaled_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572d597",
   "metadata": {},
   "source": [
    "### **Properties of Scalars:**\n",
    "\n",
    "1. **`Commutativity`**:    \n",
    "   \n",
    "   Scalar operations are commutative, meaning *$a + b = b + a$* and *$a × b = b × a$*. This property doesn't hold for matrix operations.\n",
    "\n",
    "2. **`Associativity`**: \n",
    "\n",
    "   For scalars, *$(a + b) + c = a + (b + c)$* and *$(a × b) × c = a × (b × c)$*. This allows flexible grouping in calculations.\n",
    "\n",
    "3. **`Distributivity`**:   \n",
    "   \n",
    "   Scalars distribute over addition: *$a × (b + c) = a × b + a × c$*. This property is fundamental in expanding algebraic expressions.\n",
    "\n",
    "4. **`Identity Elements`**:    \n",
    "   \n",
    "   The additive identity is 0 (*$a + 0 = a$*) and the multiplicative identity is 1 (*$a × 1 = a$*). These are crucial for maintaining values during operations.\n",
    "\n",
    "5. **`Inverse Elements`**:    \n",
    "\n",
    "   Every scalar *$a$* has an additive inverse *$-a$* such that *$a + (-a) = 0$*, and every non-zero scalar has a multiplicative inverse *$1/a$* such that *$a × (1/a) = 1$*.\n",
    "\n",
    "6. **`Scalar Multiplication with Vectors`**:   \n",
    "   \n",
    "   When multiplying a scalar with a vector, the scalar multiplies each component of the vector: *$c × [x, y, z] = [c×x, c×y, c×z]$*. This scales the vector's magnitude while preserving its direction.\n",
    "\n",
    "7. **`Scalar Multiplication with Matrices`**:    \n",
    "   \n",
    "   Similarly, scalar multiplication with matrices multiplies each element: *$c × [[a, b], [c, d]] = [[c×a, c×b], [c×c, c×d]]$*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1cf63e",
   "metadata": {},
   "source": [
    "8.  **`Field Properties`**:    \n",
    "   Scalars typically come from a field (like real numbers $ℝ$ or complex numbers $ℂ$), which means they satisfy all the algebraic properties mentioned above. This mathematical structure is what makes linear algebra operations well-defined and consistent.\n",
    "\n",
    "9. **`Dimension`**:   \n",
    "    \n",
    "   Scalars are zero-dimensional objects - they have no spatial extent, unlike vectors (1D), matrices (2D), or higher-order tensors.\n",
    "\n",
    "10. **`Invariance`**:   \n",
    "  Scalars remain unchanged under coordinate transformations. While vectors and matrices can change representation when you rotate or translate coordinate systems, scalars maintain their value.\n",
    "\n",
    "These properties make scalars the foundation for more complex linear algebra operations and are essential for understanding how deep learning algorithms manipulate data through mathematical transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183dc42",
   "metadata": {},
   "source": [
    "-------------\n",
    "-------------\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe87a5",
   "metadata": {},
   "source": [
    "## **Number Sets:**\n",
    "\n",
    "\n",
    "**1. Natural Numbers (ℕ):**\n",
    "   - **Values**: {1, 2, 3, 4, 5, 6, ...}\n",
    "   - **Description**: Positive counting numbers, sometimes includes 0 depending on context\n",
    "\n",
    "**2. Whole Numbers (𝕎):** \n",
    "  - **Values**: {0, 1, 2, 3, 4, 5, ...}\n",
    "  - **Description**: Natural numbers plus zero\n",
    "\n",
    "**3. Integers (ℤ):**\n",
    "  - **Values**: {..., -3, -2, -1, 0, 1, 2, 3, ...}\n",
    "  - **Description**: Whole numbers plus negative numbers\n",
    "\n",
    "**4. Rational Numbers (ℚ):**\n",
    "  - **Values**: All fractions p/q where p, q are integers and q ≠ 0\n",
    "  - **Examples**: 1/2, -3/4, 5, 0.25, 0.333..., -2.5\n",
    "  - **Description**: Numbers that can be expressed as fractions\n",
    "\n",
    "**5. Irrational Numbers:**\n",
    "  - **Values**: Numbers that cannot be expressed as fractions\n",
    "  - **Examples**: π, e, √2, √3, φ (golden ratio)\n",
    "  - **Description**: Non-repeating, non-terminating decimals\n",
    "\n",
    "**6. Real Numbers (ℝ):**  \n",
    "  - **Values**: All rational and irrational numbers combined\n",
    "  - **Description**: All numbers on the number line, includes everything above\n",
    "\n",
    "**7. Complex Numbers (ℂ):**\n",
    "  - **Values**: Numbers of the form a + bi where a, b are real and i = √(-1)\n",
    "  - **Examples**: 3 + 4i, -2 - 5i, 7 (pure real), 3i (pure imaginary)\n",
    "  - **Description**: Includes real numbers plus imaginary numbers\n",
    "\n",
    "**Quick Memory Aid:**\n",
    "> $ℕ ⊂ 𝕎 ⊂ ℤ ⊂ ℚ ⊂ ℝ ⊂ ℂ$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fe1ce",
   "metadata": {},
   "source": [
    "-----\n",
    "-----------\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b8231",
   "metadata": {},
   "source": [
    "## **2. Vectors:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09455842",
   "metadata": {},
   "source": [
    "> ![](https://cdn1.byjus.com/wp-content/uploads/2021/03/Vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82b25c",
   "metadata": {},
   "source": [
    "A vector is an ordered collection of numbers (called `components` or `elements`) that represents a quantity with `both magnitude and direction`. Unlike scalars which are just single numbers, vectors contain multiple values and can represent `points in space`, `directions`, or `collections of related data`.\n",
    "\n",
    "**Representation and Notation:**\n",
    "\n",
    "**Mathematical Notation**:\n",
    "   - **Bold lowercase letters**: **$v$**, **$u$**, **$a$**, **$x$**\n",
    "\n",
    "   - **Arrows**: v⃗, u⃗, a⃗\n",
    "\n",
    "   - **Component form**: $v = [v₁, v₂, v₃]$ or $v = (v₁, v₂, v₃)$\n",
    "   \n",
    "   - **Column vector**: \n",
    "  ```raw\n",
    "        v = [v₁]\n",
    "            [v₂]\n",
    "            [v₃]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021dca5",
   "metadata": {},
   "source": [
    "**Common Representations**:\n",
    "   - **2D vector**: $v = [3, 4]$ represents a point at coordinates $(3, 4)$\n",
    "\n",
    "   - **3D vector**: $v = [1, -2, 5]$ represents a point in $3D$ space\n",
    "\n",
    "   - **n-dimensional**: $v = [v₁, v₂, ..., vₙ]$ for high-dimensional spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f871d78",
   "metadata": {},
   "source": [
    "### **Geometric and Linear Algebraic Meanings:**\n",
    "\n",
    "**1. Geometric Interpretation**:  \n",
    "   - Vectors represent arrows in space with specific direction and magnitude. \n",
    "   \n",
    "   - A vector $[3, 4]$ can be visualized as an arrow starting from the origin $(0, 0)$ and pointing to the coordinate $(3, 4)$. \n",
    "   \n",
    "   - The length of this arrow is the magnitude $√(3² + 4²) = 5$, and it points in a specific direction from the origin.\n",
    "\n",
    "**2. Linear Algebraic Interpretation**:\n",
    "   - Vectors are elements of vector spaces that can be added together and multiplied by scalars. \n",
    "   \n",
    "   - They represent solutions to linear equations, transformations, and serve as building blocks for more complex mathematical structures. \n",
    "   \n",
    "   - In linear algebra, vectors are often viewed as columns or rows of numbers that can be manipulated through matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58a2ed",
   "metadata": {},
   "source": [
    "## **Usefulness in Deep Learning:**\n",
    "\n",
    "1. **`Feature Representation`**:   \n",
    "Each data sample is represented as a vector where each component represents a feature. For example, in image processing, a 28×28 pixel image becomes a 784-dimensional vector where each element represents a pixel intensity.\n",
    "\n",
    "2. **`Word Embeddings`**:   \n",
    "Words are converted into dense vectors that capture semantic meaning. Similar words have similar vector representations, enabling machines to understand relationships between words.\n",
    "\n",
    "3. **`Neural Network Weights`**:   \n",
    "Model parameters are stored as vectors, allowing efficient computation of gradients and parameter updates during training.\n",
    "\n",
    "4. **`Batch Processing`**:   \n",
    "Multiple data samples are processed simultaneously by organizing them into vectors and matrices, dramatically improving computational efficiency.\n",
    "\n",
    "5. **`Gradient Computation`**:   \n",
    "Gradients in neural networks are vectors pointing in the direction of steepest increase of the loss function, enabling optimization algorithms to update parameters effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872f44d",
   "metadata": {},
   "source": [
    "### **PyTorch Implementation Scenarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5feee8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Vector: tensor([1, 2, 3, 4, 5])\n",
      "2D Row Vector: tensor([[1, 2, 3]])\n",
      "Column Vector:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n",
      "1D Vector Shape: torch.Size([5])\n",
      "2D Row Vector Shape: torch.Size([1, 3])\n",
      "Column Vector Shape: torch.Size([3, 1])\n",
      "\n",
      "Vector Operations:\n",
      "a + b = tensor([5, 7, 9])\n",
      "a - b = tensor([-3, -3, -3])\n",
      "a · b = 32\n",
      "a × b = tensor([-3,  6, -3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ========================================\n",
    "# 1. BASIC VECTOR CREATION AND OPERATIONS\n",
    "# ========================================\n",
    "\n",
    "# Creating vectors\n",
    "vector_1d = torch.tensor([1, 2, 3, 4, 5])  # 1D vector\n",
    "vector_2d = torch.tensor([[1, 2, 3]])      # Row vector (1x3)\n",
    "vector_col = torch.tensor([[1], [2], [3]]) # Column vector (3x1)\n",
    "\n",
    "print(\"1D Vector:\", vector_1d)\n",
    "print(\"2D Row Vector:\", vector_2d)\n",
    "print(\"Column Vector:\\n\", vector_col)\n",
    "print()\n",
    "print(\"1D Vector Shape:\", vector_1d.shape)\n",
    "print(\"2D Row Vector Shape:\", vector_2d.shape)\n",
    "print(\"Column Vector Shape:\", vector_col.shape)\n",
    "\n",
    "# Vector arithmetic\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "addition = a + b           # Element-wise addition\n",
    "subtraction = a - b        # Element-wise subtraction\n",
    "dot_product = torch.dot(a, b)  # Dot product\n",
    "cross_product = torch.cross(a, b)  # Cross product (3D only)\n",
    "\n",
    "print(f\"\\nVector Operations:\")\n",
    "print(f\"a + b = {addition}\")\n",
    "print(f\"a - b = {subtraction}\")\n",
    "print(f\"a · b = {dot_product}\")\n",
    "print(f\"a × b = {cross_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20d27841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single image features:\n",
      "    tensor([0.8000, 0.6000, 0.3000, 0.7000, 0.4000, 0.9000])\n",
      "Single Image Features Shape: torch.Size([6])\n",
      "\n",
      "Image Batch features:\n",
      "    tensor([[0.8000, 0.6000, 0.3000, 0.7000, 0.4000, 0.9000],\n",
      "        [0.2000, 0.8000, 0.9000, 0.1000, 0.6000, 0.3000],\n",
      "        [0.5000, 0.4000, 0.6000, 0.8000, 0.2000, 0.7000]])\n",
      "Batch shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. FEATURE VECTORS FOR MACHINE LEARNING\n",
    "# ==========================================\n",
    "\n",
    "# Example: Image classification features\n",
    "# Simulating features extracted from an image\n",
    "image_features = torch.tensor([\n",
    "    0.8,   # brightness\n",
    "    0.6,   # contrast\n",
    "    0.3,   # red_intensity\n",
    "    0.7,   # green_intensity\n",
    "    0.4,   # blue_intensity\n",
    "    0.9    # sharpness\n",
    "])\n",
    "\n",
    "# Batch of feature vectors (multiple samples)\n",
    "batch_features = torch.tensor([\n",
    "    [0.8, 0.6, 0.3, 0.7, 0.4, 0.9],  # Image 1\n",
    "    [0.2, 0.8, 0.9, 0.1, 0.6, 0.3],  # Image 2\n",
    "    [0.5, 0.4, 0.6, 0.8, 0.2, 0.7],  # Image 3\n",
    "])\n",
    "\n",
    "print(f\"\\nSingle image features:\\n    {image_features}\")\n",
    "print(f\"Single Image Features Shape: {image_features.shape}\")\n",
    "print()\n",
    "print(f\"Image Batch features:\\n    {batch_features}\")\n",
    "print(f\"Batch shape: {batch_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46ea292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embeddings shape: torch.Size([4, 100])\n",
      "Cosine similarity between word 1 and word 2: -0.1919\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. WORD EMBEDDINGS\n",
    "# ==========================================\n",
    "\n",
    "# Creating word embeddings\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Sample word indices\n",
    "word_indices = torch.tensor([1, 15, 30, 45])\n",
    "\n",
    "# Get word vectors\n",
    "word_vectors = embedding(word_indices)\n",
    "print(f\"\\nWord embeddings shape: {word_vectors.shape}\")\n",
    "\n",
    "# Similarity between words using cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n",
    "\n",
    "similarity = cosine_similarity(word_vectors[0], word_vectors[1])\n",
    "print(f\"Cosine similarity between word 1 and word 2: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73873176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient w.r.t input: tensor([[-2.7947, -0.9862, -2.3241]])\n",
      "Weight gradients: tensor([[ -5.8407, -11.6814, -17.5221]])\n",
      "Bias gradient: tensor([-5.8407])\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. GRADIENT VECTORS\n",
    "# ==========================================\n",
    "\n",
    "# Simple neural network to demonstrate gradient vectors\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "y_true = torch.tensor([[5.0]])\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "loss = F.mse_loss(y_pred, y_true)\n",
    "\n",
    "# Backward pass - compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Access gradient vectors\n",
    "print(f\"\\nGradient w.r.t input: {x.grad}\")\n",
    "print(f\"Weight gradients: {model.linear.weight.grad}\")\n",
    "print(f\"Bias gradient: {model.linear.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b919099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vector: tensor([1., 2.])\n",
      "Transformed vector: tensor([ 5., 11.])\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. VECTOR TRANSFORMATIONS\n",
    "# ==========================================\n",
    "\n",
    "# Linear transformation using matrix multiplication\n",
    "transform_matrix = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "input_vector = torch.tensor([1, 2], dtype=torch.float32)\n",
    "\n",
    "# Apply transformation\n",
    "transformed = torch.mv(transform_matrix, input_vector)\n",
    "print(f\"\\nOriginal vector: {input_vector}\")\n",
    "print(f\"Transformed vector: {transformed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344572fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention mechanism:\n",
      "Query shape: torch.Size([5, 4])\n",
      "Attention weights shape: torch.Size([5, 5])\n",
      "Attended values shape: torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. ATTENTION MECHANISMS (QUERY, KEY, VALUE VECTORS)\n",
    "# ==========================================\n",
    "\n",
    "# Simplified attention mechanism\n",
    "seq_length = 5\n",
    "d_model = 4\n",
    "\n",
    "# Create query, key, value vectors\n",
    "queries = torch.randn(seq_length, d_model)\n",
    "keys = torch.randn(seq_length, d_model)\n",
    "values = torch.randn(seq_length, d_model)\n",
    "\n",
    "# Compute attention scores\n",
    "attention_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# Apply attention\n",
    "attended_values = torch.matmul(attention_weights, values)\n",
    "\n",
    "print(f\"\\nAttention mechanism:\")\n",
    "print(f\"Query shape: {queries.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Attended values shape: {attended_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a016e154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction probabilities:\n",
      "tensor([[0.5455, 0.0822, 0.3723],\n",
      "        [0.2699, 0.5948, 0.1352],\n",
      "        [0.6584, 0.0803, 0.2613],\n",
      "        [0.2992, 0.3410, 0.3597]])\n",
      "Cross-entropy loss: 0.9185\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 7. LOSS FUNCTION VECTORS\n",
    "# ==========================================\n",
    "\n",
    "# Multi-class classification scenario\n",
    "num_classes = 3\n",
    "batch_size = 4\n",
    "\n",
    "# Model predictions (logits)\n",
    "logits = torch.randn(batch_size, num_classes)\n",
    "# True labels\n",
    "targets = torch.tensor([0, 1, 2, 0])\n",
    "\n",
    "# Convert to probability vectors\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "print(f\"\\nPrediction probabilities:\\n{probabilities}\")\n",
    "\n",
    "# Cross-entropy loss\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(f\"Cross-entropy loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31d1db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector norms and distances:\n",
      "L1 norm of v1: 7.0\n",
      "L2 norm of v1: 5.0\n",
      "Max norm of v1: 4.0\n",
      "Euclidean distance: 2.8284270763397217\n",
      "Manhattan distance: 4.0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 8. VECTOR NORMS AND DISTANCES\n",
    "# ==========================================\n",
    "\n",
    "v1 = torch.tensor([3.0, 4.0])\n",
    "v2 = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Different norms\n",
    "l1_norm = torch.norm(v1, p=1)      # L1 norm (Manhattan distance)\n",
    "l2_norm = torch.norm(v1, p=2)      # L2 norm (Euclidean distance)\n",
    "max_norm = torch.norm(v1, p=float('inf'))  # Max norm\n",
    "\n",
    "# Distance between vectors\n",
    "euclidean_distance = torch.norm(v1 - v2)\n",
    "manhattan_distance = torch.norm(v1 - v2, p=1)\n",
    "\n",
    "print(f\"\\nVector norms and distances:\")\n",
    "print(f\"L1 norm of v1: {l1_norm}\")\n",
    "print(f\"L2 norm of v1: {l2_norm}\")\n",
    "print(f\"Max norm of v1: {max_norm}\")\n",
    "print(f\"Euclidean distance: {euclidean_distance}\")\n",
    "print(f\"Manhattan distance: {manhattan_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6aae475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vector: tensor([3., 4.])\n",
      "Unit vector: tensor([0.6000, 0.8000])\n",
      "Unit vector norm: 1.0\n",
      "\n",
      "Batch vectors shape: torch.Size([10, 5])\n",
      "Normalized batch shape: torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 9. VECTOR NORMALIZATION\n",
    "# ==========================================\n",
    "\n",
    "# Unit vector (normalized)\n",
    "unit_vector = v1 / torch.norm(v1)\n",
    "print(f\"\\nOriginal vector: {v1}\")\n",
    "print(f\"Unit vector: {unit_vector}\")\n",
    "print(f\"Unit vector norm: {torch.norm(unit_vector)}\")\n",
    "\n",
    "# Batch normalization on vectors\n",
    "batch_vectors = torch.randn(10, 5)  # 10 vectors of dimension 5\n",
    "normalized_batch = F.normalize(batch_vectors, p=2, dim=1)\n",
    "print(f\"\\nBatch vectors shape: {batch_vectors.shape}\")\n",
    "print(f\"Normalized batch shape: {normalized_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1629ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenated vector: tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Stacked vectors shape: torch.Size([3, 3])\n",
      "Reshaped to matrix:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "==================================================\n",
      "All vector operations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 10. VECTOR CONCATENATION AND RESHAPING\n",
    "# ==========================================\n",
    "\n",
    "# Concatenating vectors\n",
    "v_a = torch.tensor([1, 2, 3])\n",
    "v_b = torch.tensor([4, 5, 6])\n",
    "v_c = torch.tensor([7, 8, 9])\n",
    "\n",
    "# Concatenate along dimension 0\n",
    "concatenated = torch.cat([v_a, v_b, v_c], dim=0)\n",
    "print(f\"\\nConcatenated vector: {concatenated}\")\n",
    "\n",
    "# Stack vectors (creates new dimension)\n",
    "stacked = torch.stack([v_a, v_b, v_c], dim=0)\n",
    "print(f\"Stacked vectors shape: {stacked.shape}\")\n",
    "\n",
    "# Reshape vector\n",
    "reshaped = concatenated.view(3, 3)\n",
    "print(f\"Reshaped to matrix:\\n{reshaped}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All vector operations completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29276472",
   "metadata": {},
   "source": [
    "### **Properties of Vectors:**\n",
    "\n",
    "**Addition and Subtraction**: Vectors can be added or subtracted element-wise. For vectors **a** = [1, 2, 3] and **b** = [4, 5, 6], their sum is **a** + **b** = [5, 7, 9]. This operation is commutative (**a** + **b** = **b** + **a**) and associative ((**a** + **b**) + **c** = **a** + (**b** + **c**)).\n",
    "\n",
    "**Scalar Multiplication**: Multiplying a vector by a scalar scales all components uniformly. If **v** = [2, 3, 4] and scalar c = 3, then c**v** = [6, 9, 12]. This operation preserves the direction of the vector but changes its magnitude.\n",
    "\n",
    "**Dot Product (Inner Product)**: The dot product of two vectors produces a scalar value. For **a** = [1, 2, 3] and **b** = [4, 5, 6], the dot product is **a** · **b** = 1×4 + 2×5 + 3×6 = 32. This operation is commutative and measures the similarity between vectors.\n",
    "\n",
    "**Cross Product**: In 3D space, the cross product of two vectors produces a third vector perpendicular to both original vectors. For **a** = [1, 2, 3] and **b** = [4, 5, 6], **a** × **b** = [2×6 - 3×5, 3×4 - 1×6, 1×5 - 2×4] = [-3, 6, -3].\n",
    "\n",
    "**Magnitude (Norm)**: The magnitude of a vector represents its length. For vector **v** = [3, 4], the magnitude is ||**v**|| = √(3² + 4²) = 5. Different norms exist: L1 norm (Manhattan distance), L2 norm (Euclidean distance), and infinity norm (maximum component).\n",
    "\n",
    "**Unit Vectors**: A unit vector has magnitude 1 and represents pure direction. Any vector can be normalized to create a unit vector by dividing by its magnitude: **û** = **v**/||**v**||.\n",
    "\n",
    "**Linear Independence**: A set of vectors is linearly independent if none can be expressed as a linear combination of the others. This property is crucial for determining the dimension of vector spaces and the uniqueness of solutions to linear systems.\n",
    "\n",
    "**Orthogonality**: Two vectors are orthogonal if their dot product is zero, meaning they're perpendicular. Orthogonal vectors are particularly useful in machine learning for creating uncorrelated features and in optimization algorithms.\n",
    "\n",
    "**Span**: The span of a set of vectors is the collection of all possible linear combinations of those vectors. It represents the space that can be reached by combining the vectors with different scalar weights.\n",
    "\n",
    "**Dimensionality**: The number of components in a vector determines its dimensionality. A 3D vector has three components, while high-dimensional vectors in machine learning can have thousands or millions of components.\n",
    "\n",
    "**Basis Vectors**: A basis is a set of linearly independent vectors that span the entire vector space. Any vector in the space can be uniquely expressed as a linear combination of basis vectors.\n",
    "\n",
    "**Distance and Similarity**: Vectors can be compared using various distance metrics. Euclidean distance measures straight-line distance, while cosine similarity measures the angle between vectors, making it useful for comparing directions regardless of magnitude.\n",
    "\n",
    "These properties make vectors fundamental to machine learning, computer graphics, physics simulations, and many other computational applications. Understanding these concepts is essential for working with neural networks, where data flows through layers as vectors and matrices, and optimization algorithms update parameter vectors based on gradient vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
