{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850a7b5b",
   "metadata": {},
   "source": [
    "# **Vector Operations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b11ee8",
   "metadata": {},
   "source": [
    "## **1. Vector Addition:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2cdd63",
   "metadata": {},
   "source": [
    "Vector addition is the element-wise addition of two or more tensors of the same shape, where corresponding elements are added together.\n",
    "\n",
    "For two vectors **a** = $[a₁, a₂, ..., aₙ]$ and **b** = $[b₁, b₂, ..., bₙ]$:\n",
    "\n",
    "> $a + b = [a₁ + b₁, a₂ + b₂, ..., aₙ + bₙ]$\n",
    "\n",
    "**Properties:**\n",
    "   - Commutative: **$a + b = b + a$**\n",
    "   -  Associative: **$(a + b) + c = a + (b + c)$**\n",
    "   - Identity: **$a + 0 = a$**\n",
    "   - Inverse: **$a + (-a) = 0$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af0b6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = tensor([ 6,  8, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Method 1: Using + operator\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = torch.tensor([5, 6, 7, 8])\n",
    "result1 = a + b\n",
    "print(f\"a + b = {result1}\")  # [6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b476ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.add(a, b) = tensor([ 6,  8, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using torch.add()\n",
    "result2 = torch.add(a, b)\n",
    "print(f\"torch.add(a, b) = {result2}\")  # [6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbd39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a += b = tensor([ 6,  8, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "# Method 3: In-place addition with +=\n",
    "a_copy = a.clone()\n",
    "a_copy += b\n",
    "print(f\"a += b = {a_copy}\")  # [6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96f8abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.add_(b) = tensor([ 6,  8, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "# Method 4: In-place addition with add_()\n",
    "a_copy2 = a.clone()\n",
    "a_copy2.add_(b)\n",
    "print(f\"a.add_(b) = {a_copy2}\")  # [6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5312e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + scalar = tensor([11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "# Method 5: Adding scalar to vector\n",
    "scalar = 10\n",
    "result3 = a + scalar\n",
    "print(f\"a + scalar = {result3}\")  # [11, 12, 13, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b48f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b + c = tensor([ 7,  9, 11, 13])\n"
     ]
    }
   ],
   "source": [
    "# Method 6: Adding multiple vectors\n",
    "c = torch.tensor([1, 1, 1, 1])\n",
    "result4 = a + b + c\n",
    "print(f\"a + b + c = {result4}\")  # [7, 9, 11, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3dc482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D + 1D broadcasting:\n",
      "tensor([[11, 22],\n",
      "        [13, 24]])\n"
     ]
    }
   ],
   "source": [
    "# Method 7: Broadcasting addition (different shapes)\n",
    "a_2d = torch.tensor([[1, 2], [3, 4]])\n",
    "b_1d = torch.tensor([10, 20])\n",
    "result5 = a_2d + b_1d  # Broadcasting\n",
    "print(f\"2D + 1D broadcasting:\\n{result5}\")\n",
    "# [[11, 22]\n",
    "#  [13, 24]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898b1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + 5*b = tensor([26, 32, 38, 44])\n"
     ]
    }
   ],
   "source": [
    "# Method 8: Weighted addition\n",
    "alpha = 5\n",
    "beta = 0.3\n",
    "result6 = torch.add(a, b, alpha=alpha)  # a + alpha * b\n",
    "print(f\"a + 5*b = {result6}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd9c8b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float + int = tensor([2., 4., 6.])\n",
      "Result dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Method 9: Adding with different dtypes (automatic type promotion)\n",
    "a_float = torch.tensor([1.0, 2.0, 3.0])\n",
    "b_int = torch.tensor([1, 2, 3])\n",
    "result7 = a_float + b_int\n",
    "print(f\"float + int = {result7}\")  # [2.0, 4.0, 6.0]\n",
    "print(f\"Result dtype: {result7.dtype}\")  # torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a6f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 10: GPU vector addition (if CUDA available)\n",
    "if torch.cuda.is_available():\n",
    "    a_gpu = torch.tensor([1, 2, 3, 4]).cuda()\n",
    "    b_gpu = torch.tensor([5, 6, 7, 8]).cuda()\n",
    "    result_gpu = a_gpu + b_gpu\n",
    "    print(f\"GPU addition: {result_gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a78f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch_a: torch.Size([32, 128])\n",
      "Shape of batch_b: torch.Size([32, 128])\n",
      "Batch addition shape: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "# Method 11: Batch vector addition\n",
    "\n",
    "batch_a = torch.randn(32, 128)  # 32 vectors of dimension 128\n",
    "batch_b = torch.randn(32, 128)\n",
    "batch_result = batch_a + batch_b\n",
    "print(f\"Shape of batch_a: {batch_a.shape}\")\n",
    "print(f\"Shape of batch_b: {batch_b.shape}\")\n",
    "print(f\"Batch addition shape: {batch_result.shape}\")  # torch.Size([32, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3efb2af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a: tensor([1, 2, 3, 4])\n",
      "Print b: tensor([5, 6, 7, 8])\n",
      "Print output: tensor([ 6,  8, 10, 12])\n",
      "shape of output: torch.Size([4])\n",
      "Addition with output tensor: tensor([ 6,  8, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "# Method 12: Element-wise addition with specific output tensor\n",
    "\n",
    "output = torch.empty_like(a)\n",
    "print(f\"Print a: {a}\")\n",
    "print(f\"Print b: {b}\")\n",
    "torch.add(a, b, out=output)\n",
    "print(f\"Print output: {output}\")\n",
    "print(f\"shape of output: {output.shape}\")\n",
    "print(f\"Addition with output tensor: {output}\")  # [6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aefe8f",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "   1. **Broadcasting**: PyTorch automatically handles different tensor shapes when possible\n",
    "\n",
    "   2. **In-place operations**: Methods ending with `_` modify the original tensor\n",
    "\n",
    "   3. **Type promotion**: PyTorch automatically promotes to the most general dtype\n",
    "\n",
    "   4. **GPU support**: Operations work seamlessly on CUDA tensors\n",
    "\n",
    "   5. **Batch operations**: Addition works efficiently on batched data\n",
    "\n",
    "   6. **Memory efficiency**: Can specify output tensor to avoid extra memory allocation\n",
    "\n",
    "**Common Use Cases in Deep Learning:**\n",
    "   - Adding bias terms to linear layers\n",
    "\n",
    "   - Residual connections in neural networks\n",
    "\n",
    "   - Combining feature representations\n",
    "\n",
    "   - Gradient accumulation during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60106ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d92733",
   "metadata": {},
   "source": [
    "## **2. Broadcasting:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1f885",
   "metadata": {},
   "source": [
    "> ![](https://s3.amazonaws.com/cloudxlab/static/images/course/numpy_pandas_for_ml/Broacasting_2_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab4159",
   "metadata": {},
   "source": [
    "Broadcasting refers to the automatic expansion of arrays or tensors with different shapes to make them compatible for element-wise operations. It's a computational technique that allows operations between arrays of different dimensions without explicitly reshaping them.\n",
    "\n",
    "#### **Rules of Broadcasting:**\n",
    "\n",
    "**Broadcasting follows these fundamental rules:**\n",
    "\n",
    "1. **`Rule 1`: Dimension Alignment:**\n",
    "Arrays are aligned from the rightmost dimension (trailing dimensions). If arrays have different numbers of dimensions, the smaller array is conceptually padded with dimensions of size 1 on the left.\n",
    "\n",
    "2. **`Rule 2`: Dimension Compatibility**\n",
    "Two dimensions are compatible if:\n",
    "- They are equal in size\n",
    "- One of them is 1\n",
    "- One of them is missing (treated as 1)\n",
    "\n",
    "3. **`Rule 3`: Result Shape**\n",
    "The resulting array has the maximum size along each dimension from the input arrays.\n",
    "\n",
    "4. **`Rule 4`: Singleton Expansion**\n",
    "Dimensions of size 1 are \"stretched\" or \"copied\" to match the corresponding dimension of the other array.\n",
    "\n",
    "#### **Broadcasting in PyTorch:**\n",
    "\n",
    "**1. Implicit Broadcasting:**\n",
    "\n",
    "PyTorch automatically applies broadcasting rules during element-wise operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3d3ff",
   "metadata": {},
   "source": [
    "> ![](https://miro.medium.com/v2/resize:fit:1057/0*_MANoFD5glrde0eh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37e281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example 1: Vector + Scalar\n",
    "a = torch.tensor([1, 2, 3])  # shape: (3,) # Vector \n",
    "b = torch.tensor(5)          # shape: () # Scalar \n",
    "result = a + b               # shape: (3,) -> [6, 7, 8] # Vector \n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d4e139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Vector: tensor([10, 20, 30])\n",
      "\n",
      "Result:\n",
      " tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Matrix + Vector\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])  # shape: (2, 3)\n",
    "\n",
    "vector = torch.tensor([10, 20, 30])  # shape: (3,)\n",
    "print(f\"Matrix:\\n {matrix}\")\n",
    "print()\n",
    "print(f\"Vector: {vector}\")\n",
    "print()\n",
    "result = matrix + vector     # shape: (2, 3) -> [[11, 22, 33], [14, 25, 36]]\n",
    "print(f\"Result:\\n {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62072d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[[1, 2]]])\n",
      "\n",
      "B: tensor([[3],\n",
      "        [4]])\n",
      "\n",
      "Result:\n",
      " tensor([[[4, 5],\n",
      "         [5, 6]]])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Different dimensional arrays\n",
    "a = torch.tensor([[[1, 2]]])     # shape: (1, 1, 2)\n",
    "b = torch.tensor([[3], [4]])     # shape: (2, 1)\n",
    "result = a + b                   # shape: (1, 2, 2) -> [[[4, 5], [5, 6]]]\n",
    "print(f\"A: {a}\")\n",
    "print()\n",
    "print(f\"B: {b}\")\n",
    "print()\n",
    "print(f\"Result:\\n {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10887a05",
   "metadata": {},
   "source": [
    "2. **Explicit Broadcasting:**\n",
    "\n",
    "We can manually control broadcasting using specific PyTorch functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2517402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[1, 2, 3]])\n",
      "B:\n",
      " tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# Using expand() - creates a view without copying data\n",
    "a = torch.tensor([[1, 2, 3]])    # shape: (1, 3)\n",
    "b = a.expand(4, 3)               # shape: (4, 3) - same data, different view\n",
    "print(f\"A: {a}\")\n",
    "print(f\"B:\\n {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70738ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\n",
      " tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# Using repeat() - actually copies data\n",
    "c = a.repeat(4, 1)               # shape: (4, 3) - data is copied\n",
    "print(f'C:\\n {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d83138da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: tensor([1, 2, 3])\n",
      "Column:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "Row: tensor([[1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# Using unsqueeze() to add dimensions\n",
    "vector = torch.tensor([1, 2, 3])         # shape: (3,)\n",
    "column = vector.unsqueeze(1)             # shape: (3, 1)\n",
    "row = vector.unsqueeze(0)                # shape: (1, 3)\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"Column:\\n {column}\")\n",
    "print(f\"Row: {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d772bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: tensor([1, 2, 3, 4])\n",
      "\n",
      "Reshaped:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Using view() to reshape\n",
    "d = torch.tensor([1, 2, 3, 4])\n",
    "reshaped = d.view(2, 2)                  # shape: (2, 2)\n",
    "print(f\"D: {d}\")\n",
    "print(f\"\\nReshaped:\\n {reshaped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4a1dc",
   "metadata": {},
   "source": [
    "**Example 1: Matrix-Vector Operations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "994b4a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:\n",
      " tensor([[-0.3928, -1.3567,  0.5808, -0.4315],\n",
      "        [ 0.0587, -1.4496,  0.8147, -0.7282],\n",
      "        [ 1.2816,  0.5309, -1.1299, -0.0172]])\n",
      "\n",
      "Vector: tensor([ 0.6906, -0.5814,  0.4244, -0.6922])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting a vector across matrix rows\n",
    "matrix = torch.randn(3, 4)      # shape: (3, 4)\n",
    "vector = torch.randn(4)         # shape: (4,)\n",
    "print(f\"Matrix:\\n {matrix}\")\n",
    "print(f\"\\nVector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbff3445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result-1:\n",
      " tensor([[ 0.2977, -1.9382,  1.0052, -1.1237],\n",
      "        [ 0.7493, -2.0310,  1.2391, -1.4204],\n",
      "        [ 1.9722, -0.0505, -0.7055, -0.7094]])\n",
      "\n",
      "Vector Expanded:\n",
      " tensor([[ 0.6906, -0.5814,  0.4244, -0.6922],\n",
      "        [ 0.6906, -0.5814,  0.4244, -0.6922],\n",
      "        [ 0.6906, -0.5814,  0.4244, -0.6922]])\n",
      "\n",
      "Result-2:\n",
      " tensor([[ 0.2977, -1.9382,  1.0052, -1.1237],\n",
      "        [ 0.7493, -2.0310,  1.2391, -1.4204],\n",
      "        [ 1.9722, -0.0505, -0.7055, -0.7094]])\n"
     ]
    }
   ],
   "source": [
    "# Implicit broadcasting\n",
    "result1 = matrix + vector       # vector broadcasts to (3, 4)\n",
    "print(f\"Result-1:\\n {result1}\")\n",
    "# Explicit broadcasting\n",
    "vector_expanded = vector.expand(3, 4)\n",
    "print(f\"\\nVector Expanded:\\n {vector_expanded}\")\n",
    "result2 = matrix + vector_expanded\n",
    "print(f\"\\nResult-2:\\n {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cd9a0",
   "metadata": {},
   "source": [
    "**Example 2: Batch Operations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6c3382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Data: tensor([[[ 8.4271e-01, -6.7706e-01,  1.4531e+00,  8.2296e-01,  1.1806e-01],\n",
      "         [-7.8966e-01, -1.1944e+00, -3.5504e-01,  1.5645e+00, -1.5428e+00],\n",
      "         [ 1.2981e-01,  7.0207e-01,  4.3856e-01,  4.3933e-01, -1.8242e-01],\n",
      "         ...,\n",
      "         [-1.1816e+00, -2.0656e+00, -1.0027e+00, -8.8516e-01,  1.2360e+00],\n",
      "         [ 7.8958e-01,  1.9610e-01, -8.8837e-01,  1.2824e+00,  9.3674e-01],\n",
      "         [ 3.8156e-01,  8.6027e-01,  3.2817e-01,  5.9188e-01, -1.9503e+00]],\n",
      "\n",
      "        [[-1.5584e+00, -1.2375e+00, -3.2094e-01,  6.6372e-01,  1.5936e+00],\n",
      "         [ 6.7490e-01,  2.7213e-01,  1.8411e+00, -1.4736e+00, -3.2531e-01],\n",
      "         [ 9.6957e-02,  9.9925e-01, -1.5838e-01,  1.3388e+00,  3.9225e-01],\n",
      "         ...,\n",
      "         [-4.7506e-01, -2.6398e-01,  2.5711e-01,  5.5053e-01,  2.9393e-01],\n",
      "         [ 7.6167e-01,  6.3414e-01, -6.2646e-01,  2.5528e-01, -1.1959e-01],\n",
      "         [-2.8540e-01, -4.5929e-01, -4.1384e-01,  6.1178e-01,  1.1477e+00]],\n",
      "\n",
      "        [[ 1.2827e+00,  1.3064e+00, -7.5187e-02,  1.1829e+00, -1.2247e+00],\n",
      "         [ 7.3274e-01, -2.9280e-01,  8.0201e-01, -2.6995e-03, -2.8254e-01],\n",
      "         [ 1.4234e+00, -7.5238e-01, -2.7636e-01,  1.2262e+00, -9.9176e-02],\n",
      "         ...,\n",
      "         [-6.7282e-01,  2.9371e-01, -1.1402e+00,  1.5470e+00,  1.7470e+00],\n",
      "         [-8.5289e-01,  2.2410e+00,  2.1697e-02, -2.2070e+00,  7.7078e-01],\n",
      "         [ 2.7454e-01, -8.0388e-01,  5.1356e-01,  2.8408e+00,  9.9641e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.1970e-01, -8.8318e-01, -2.2890e-01,  6.9126e-02,  1.0207e+00],\n",
      "         [-7.6525e-01, -3.3313e-01, -1.0984e+00,  8.8257e-01,  6.9333e-01],\n",
      "         [-7.2734e-01, -1.6177e-01,  3.9587e-01,  6.5756e-01,  6.1399e-01],\n",
      "         ...,\n",
      "         [-1.1693e+00,  1.7552e+00, -3.8600e-01, -4.4755e-01, -1.3994e+00],\n",
      "         [-1.8613e+00, -7.3133e-01, -3.1814e-02,  1.4703e-01,  6.4928e-01],\n",
      "         [ 3.2499e-01, -2.9247e+00, -2.3705e-01,  1.6169e+00,  9.2228e-01]],\n",
      "\n",
      "        [[ 8.4032e-01, -9.2078e-01, -2.1879e-01,  6.0172e-01,  6.6914e-01],\n",
      "         [ 3.3047e-01, -5.8532e-01,  6.4281e-01, -1.0123e-01,  6.9014e-01],\n",
      "         [ 6.8632e-01,  8.6084e-01,  2.6546e-03,  1.1300e-01,  1.4900e+00],\n",
      "         ...,\n",
      "         [-1.8702e-01,  1.7782e+00, -8.5597e-01, -2.2214e+00, -1.0217e+00],\n",
      "         [-1.7586e+00,  2.8380e-01,  1.0160e-02, -2.2608e-01,  9.8471e-01],\n",
      "         [ 3.1689e-02, -1.5604e+00,  9.0013e-01,  1.3168e-01,  9.1112e-01]],\n",
      "\n",
      "        [[-4.1715e-01,  2.3928e-01,  1.5174e-01, -1.4490e+00,  2.8900e-01],\n",
      "         [-1.4975e+00, -1.8478e+00,  5.0848e-01,  1.9609e-01, -4.6748e-01],\n",
      "         [ 6.3326e-01,  4.1674e-01, -1.9192e-01, -1.3628e+00, -3.8385e-01],\n",
      "         ...,\n",
      "         [ 1.2631e+00, -3.4900e-01, -4.9267e-01,  9.1231e-01, -3.9353e-02],\n",
      "         [-1.2815e+00, -4.1938e-01, -8.0198e-01, -2.0189e-01, -1.2228e+00],\n",
      "         [-7.1012e-01,  1.2533e+00,  4.8072e-01, -1.0348e-01,  4.3204e-01]]])\n",
      "--------------------------------------------------------------------------------\n",
      "Mean: tensor([-0.2648,  0.4224, -1.4129, -2.2198, -0.4862])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting in batch processing\n",
    "batch_data = torch.randn(32, 10, 5)    # 32 samples, 10 features, 5 dimensions\n",
    "mean = torch.randn(5)                  # mean for each dimension\n",
    "print(f\"Batch Data: {batch_data}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Mean: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f305cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized:\n",
      " tensor([[[ 1.1075e+00, -1.0995e+00,  2.8660e+00,  3.0428e+00,  6.0431e-01],\n",
      "         [-5.2487e-01, -1.6168e+00,  1.0579e+00,  3.7843e+00, -1.0565e+00],\n",
      "         [ 3.9460e-01,  2.7966e-01,  1.8515e+00,  2.6592e+00,  3.0382e-01],\n",
      "         ...,\n",
      "         [-9.1676e-01, -2.4880e+00,  4.1024e-01,  1.3347e+00,  1.7222e+00],\n",
      "         [ 1.0544e+00, -2.2630e-01,  5.2454e-01,  3.5022e+00,  1.4230e+00],\n",
      "         [ 6.4636e-01,  4.3787e-01,  1.7411e+00,  2.8117e+00, -1.4641e+00]],\n",
      "\n",
      "        [[-1.2936e+00, -1.6599e+00,  1.0920e+00,  2.8836e+00,  2.0798e+00],\n",
      "         [ 9.3969e-01, -1.5027e-01,  3.2540e+00,  7.4624e-01,  1.6094e-01],\n",
      "         [ 3.6175e-01,  5.7684e-01,  1.2545e+00,  3.5586e+00,  8.7850e-01],\n",
      "         ...,\n",
      "         [-2.1026e-01, -6.8638e-01,  1.6700e+00,  2.7704e+00,  7.8017e-01],\n",
      "         [ 1.0265e+00,  2.1174e-01,  7.8645e-01,  2.4751e+00,  3.6666e-01],\n",
      "         [-2.0606e-02, -8.8169e-01,  9.9908e-01,  2.8316e+00,  1.6340e+00]],\n",
      "\n",
      "        [[ 1.5475e+00,  8.8402e-01,  1.3377e+00,  3.4027e+00, -7.3844e-01],\n",
      "         [ 9.9753e-01, -7.1520e-01,  2.2149e+00,  2.2171e+00,  2.0371e-01],\n",
      "         [ 1.6882e+00, -1.1748e+00,  1.1366e+00,  3.4460e+00,  3.8707e-01],\n",
      "         ...,\n",
      "         [-4.0803e-01, -1.2869e-01,  2.7276e-01,  3.7668e+00,  2.2332e+00],\n",
      "         [-5.8810e-01,  1.8186e+00,  1.4346e+00,  1.2851e-02,  1.2570e+00],\n",
      "         [ 5.3933e-01, -1.2263e+00,  1.9265e+00,  5.0607e+00,  1.4827e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.5490e-01, -1.3056e+00,  1.1840e+00,  2.2890e+00,  1.5069e+00],\n",
      "         [-5.0046e-01, -7.5554e-01,  3.1448e-01,  3.1024e+00,  1.1796e+00],\n",
      "         [-4.6255e-01, -5.8417e-01,  1.8088e+00,  2.8774e+00,  1.1002e+00],\n",
      "         ...,\n",
      "         [-9.0449e-01,  1.3328e+00,  1.0269e+00,  1.7723e+00, -9.1314e-01],\n",
      "         [-1.5965e+00, -1.1537e+00,  1.3811e+00,  2.3669e+00,  1.1355e+00],\n",
      "         [ 5.8978e-01, -3.3471e+00,  1.1759e+00,  3.8367e+00,  1.4085e+00]],\n",
      "\n",
      "        [[ 1.1051e+00, -1.3432e+00,  1.1941e+00,  2.8216e+00,  1.1554e+00],\n",
      "         [ 5.9526e-01, -1.0077e+00,  2.0557e+00,  2.1186e+00,  1.1764e+00],\n",
      "         [ 9.5111e-01,  4.3843e-01,  1.4156e+00,  2.3328e+00,  1.9763e+00],\n",
      "         ...,\n",
      "         [ 7.7774e-02,  1.3558e+00,  5.5695e-01, -1.5187e-03, -5.3543e-01],\n",
      "         [-1.4938e+00, -1.3860e-01,  1.4231e+00,  1.9938e+00,  1.4710e+00],\n",
      "         [ 2.9648e-01, -1.9828e+00,  2.3130e+00,  2.3515e+00,  1.3974e+00]],\n",
      "\n",
      "        [[-1.5236e-01, -1.8312e-01,  1.5647e+00,  7.7089e-01,  7.7525e-01],\n",
      "         [-1.2327e+00, -2.2702e+00,  1.9214e+00,  2.4159e+00,  1.8765e-02],\n",
      "         [ 8.9805e-01, -5.6656e-03,  1.2210e+00,  8.5701e-01,  1.0240e-01],\n",
      "         ...,\n",
      "         [ 1.5279e+00, -7.7140e-01,  9.2025e-01,  3.1322e+00,  4.4689e-01],\n",
      "         [-1.0167e+00, -8.4179e-01,  6.1093e-01,  2.0180e+00, -7.3656e-01],\n",
      "         [-4.4533e-01,  8.3089e-01,  1.8936e+00,  2.1164e+00,  9.1829e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# Subtract mean from each sample (implicit broadcasting)\n",
    "normalized = batch_data - mean         # mean broadcasts to (32, 10, 5)\n",
    "print(f\"Normalized:\\n {normalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a71614a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1075e+00, -1.0995e+00,  2.8660e+00,  3.0428e+00,  6.0431e-01],\n",
       "         [-5.2487e-01, -1.6168e+00,  1.0579e+00,  3.7843e+00, -1.0565e+00],\n",
       "         [ 3.9460e-01,  2.7966e-01,  1.8515e+00,  2.6592e+00,  3.0382e-01],\n",
       "         ...,\n",
       "         [-9.1676e-01, -2.4880e+00,  4.1024e-01,  1.3347e+00,  1.7222e+00],\n",
       "         [ 1.0544e+00, -2.2630e-01,  5.2454e-01,  3.5022e+00,  1.4230e+00],\n",
       "         [ 6.4636e-01,  4.3787e-01,  1.7411e+00,  2.8117e+00, -1.4641e+00]],\n",
       "\n",
       "        [[-1.2936e+00, -1.6599e+00,  1.0920e+00,  2.8836e+00,  2.0798e+00],\n",
       "         [ 9.3969e-01, -1.5027e-01,  3.2540e+00,  7.4624e-01,  1.6094e-01],\n",
       "         [ 3.6175e-01,  5.7684e-01,  1.2545e+00,  3.5586e+00,  8.7850e-01],\n",
       "         ...,\n",
       "         [-2.1026e-01, -6.8638e-01,  1.6700e+00,  2.7704e+00,  7.8017e-01],\n",
       "         [ 1.0265e+00,  2.1174e-01,  7.8645e-01,  2.4751e+00,  3.6666e-01],\n",
       "         [-2.0606e-02, -8.8169e-01,  9.9908e-01,  2.8316e+00,  1.6340e+00]],\n",
       "\n",
       "        [[ 1.5475e+00,  8.8402e-01,  1.3377e+00,  3.4027e+00, -7.3844e-01],\n",
       "         [ 9.9753e-01, -7.1520e-01,  2.2149e+00,  2.2171e+00,  2.0371e-01],\n",
       "         [ 1.6882e+00, -1.1748e+00,  1.1366e+00,  3.4460e+00,  3.8707e-01],\n",
       "         ...,\n",
       "         [-4.0803e-01, -1.2869e-01,  2.7276e-01,  3.7668e+00,  2.2332e+00],\n",
       "         [-5.8810e-01,  1.8186e+00,  1.4346e+00,  1.2851e-02,  1.2570e+00],\n",
       "         [ 5.3933e-01, -1.2263e+00,  1.9265e+00,  5.0607e+00,  1.4827e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.5490e-01, -1.3056e+00,  1.1840e+00,  2.2890e+00,  1.5069e+00],\n",
       "         [-5.0046e-01, -7.5554e-01,  3.1448e-01,  3.1024e+00,  1.1796e+00],\n",
       "         [-4.6255e-01, -5.8417e-01,  1.8088e+00,  2.8774e+00,  1.1002e+00],\n",
       "         ...,\n",
       "         [-9.0449e-01,  1.3328e+00,  1.0269e+00,  1.7723e+00, -9.1314e-01],\n",
       "         [-1.5965e+00, -1.1537e+00,  1.3811e+00,  2.3669e+00,  1.1355e+00],\n",
       "         [ 5.8978e-01, -3.3471e+00,  1.1759e+00,  3.8367e+00,  1.4085e+00]],\n",
       "\n",
       "        [[ 1.1051e+00, -1.3432e+00,  1.1941e+00,  2.8216e+00,  1.1554e+00],\n",
       "         [ 5.9526e-01, -1.0077e+00,  2.0557e+00,  2.1186e+00,  1.1764e+00],\n",
       "         [ 9.5111e-01,  4.3843e-01,  1.4156e+00,  2.3328e+00,  1.9763e+00],\n",
       "         ...,\n",
       "         [ 7.7774e-02,  1.3558e+00,  5.5695e-01, -1.5187e-03, -5.3543e-01],\n",
       "         [-1.4938e+00, -1.3860e-01,  1.4231e+00,  1.9938e+00,  1.4710e+00],\n",
       "         [ 2.9648e-01, -1.9828e+00,  2.3130e+00,  2.3515e+00,  1.3974e+00]],\n",
       "\n",
       "        [[-1.5236e-01, -1.8312e-01,  1.5647e+00,  7.7089e-01,  7.7525e-01],\n",
       "         [-1.2327e+00, -2.2702e+00,  1.9214e+00,  2.4159e+00,  1.8765e-02],\n",
       "         [ 8.9805e-01, -5.6656e-03,  1.2210e+00,  8.5701e-01,  1.0240e-01],\n",
       "         ...,\n",
       "         [ 1.5279e+00, -7.7140e-01,  9.2025e-01,  3.1322e+00,  4.4689e-01],\n",
       "         [-1.0167e+00, -8.4179e-01,  6.1093e-01,  2.0180e+00, -7.3656e-01],\n",
       "         [-4.4533e-01,  8.3089e-01,  1.8936e+00,  2.1164e+00,  9.1829e-01]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicit approach\n",
    "mean_expanded = mean.expand(32, 10, 5)\n",
    "normalized_explicit = batch_data - mean_expanded\n",
    "normalized_explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c055d",
   "metadata": {},
   "source": [
    "**Example 3: Complex Broadcasting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6eb0fbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5980e-01, -7.3781e-01,  2.2131e-02,  1.3499e-01,  1.1594e+00],\n",
       "          [ 9.9263e-01, -1.0498e-01,  6.5496e-01,  7.6781e-01,  1.7922e+00],\n",
       "          [ 8.5279e-01, -2.4482e-01,  5.1512e-01,  6.2797e-01,  1.6524e+00],\n",
       "          [ 1.0398e+00, -5.7785e-02,  7.0215e-01,  8.1501e-01,  1.8394e+00],\n",
       "          [ 2.0028e+00,  9.0524e-01,  1.6652e+00,  1.7780e+00,  2.8024e+00],\n",
       "          [ 1.9342e+00,  8.3655e-01,  1.5965e+00,  1.7093e+00,  2.7337e+00]],\n",
       "\n",
       "         [[ 6.5613e-02, -1.0210e+00,  3.0229e-01, -6.1205e-01, -7.0319e-01],\n",
       "          [ 6.9844e-01, -3.8816e-01,  9.3511e-01,  2.0773e-02, -7.0364e-02],\n",
       "          [ 5.5860e-01, -5.2799e-01,  7.9527e-01, -1.1906e-01, -2.1020e-01],\n",
       "          [ 7.4564e-01, -3.4096e-01,  9.8231e-01,  6.7970e-02, -2.3166e-02],\n",
       "          [ 1.7087e+00,  6.2206e-01,  1.9453e+00,  1.0310e+00,  9.3986e-01],\n",
       "          [ 1.6400e+00,  5.5338e-01,  1.8766e+00,  9.6231e-01,  8.7117e-01]],\n",
       "\n",
       "         [[-1.3048e+00,  7.4937e-01, -7.0184e-01, -1.0913e+00,  1.2733e+00],\n",
       "          [-6.7194e-01,  1.3822e+00, -6.9018e-02, -4.5844e-01,  1.9061e+00],\n",
       "          [-8.1178e-01,  1.2424e+00, -2.0886e-01, -5.9828e-01,  1.7663e+00],\n",
       "          [-6.2474e-01,  1.4294e+00, -2.1820e-02, -4.1124e-01,  1.9533e+00],\n",
       "          [ 3.3828e-01,  2.3924e+00,  9.4120e-01,  5.5178e-01,  2.9163e+00],\n",
       "          [ 2.6960e-01,  2.3237e+00,  8.7252e-01,  4.8310e-01,  2.8477e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.0302e-01, -2.6298e+00, -1.7082e+00, -6.0915e-01, -4.5963e-01],\n",
       "          [ 2.2981e-01, -1.9969e+00, -1.0754e+00,  2.3670e-02,  1.7320e-01],\n",
       "          [ 8.9971e-02, -2.1368e+00, -1.2152e+00, -1.1617e-01,  3.3358e-02],\n",
       "          [ 2.7701e-01, -1.9497e+00, -1.0282e+00,  7.0868e-02,  2.2039e-01],\n",
       "          [ 1.2400e+00, -9.8673e-01, -6.5175e-02,  1.0339e+00,  1.1834e+00],\n",
       "          [ 1.1713e+00, -1.0554e+00, -1.3386e-01,  9.6521e-01,  1.1147e+00]],\n",
       "\n",
       "         [[-7.1175e-01, -9.3521e-01, -1.2036e+00, -1.8519e+00, -5.0273e-01],\n",
       "          [-7.8924e-02, -3.0238e-01, -5.7077e-01, -1.2190e+00,  1.3009e-01],\n",
       "          [-2.1876e-01, -4.4222e-01, -7.1061e-01, -1.3589e+00, -9.7434e-03],\n",
       "          [-3.1726e-02, -2.5518e-01, -5.2357e-01, -1.1718e+00,  1.7729e-01],\n",
       "          [ 9.3130e-01,  7.0784e-01,  4.3945e-01, -2.0881e-01,  1.1403e+00],\n",
       "          [ 8.6261e-01,  6.3916e-01,  3.7077e-01, -2.7750e-01,  1.0716e+00]],\n",
       "\n",
       "         [[-2.4920e+00, -9.4887e-01, -7.7865e-01,  2.6109e-01,  1.1725e+00],\n",
       "          [-1.8592e+00, -3.1604e-01, -1.4582e-01,  8.9391e-01,  1.8053e+00],\n",
       "          [-1.9991e+00, -4.5588e-01, -2.8566e-01,  7.5408e-01,  1.6655e+00],\n",
       "          [-1.8120e+00, -2.6885e-01, -9.8625e-02,  9.4111e-01,  1.8525e+00],\n",
       "          [-8.4901e-01,  6.9418e-01,  8.6440e-01,  1.9041e+00,  2.8155e+00],\n",
       "          [-9.1769e-01,  6.2549e-01,  7.9571e-01,  1.8354e+00,  2.7468e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6605e+00,  5.6287e-01,  1.3228e+00,  1.4357e+00,  2.4600e+00],\n",
       "          [-9.0547e-01, -2.0031e+00, -1.2431e+00, -1.1303e+00, -1.0590e-01],\n",
       "          [ 8.7282e-01, -2.2479e-01,  5.3515e-01,  6.4800e-01,  1.6724e+00],\n",
       "          [ 2.1717e-01, -8.8044e-01, -1.2050e-01, -7.6486e-03,  1.0167e+00],\n",
       "          [ 2.4906e+00,  1.3930e+00,  2.1529e+00,  2.2658e+00,  3.2901e+00],\n",
       "          [-5.0742e-01, -1.6050e+00, -8.4509e-01, -7.3223e-01,  2.9215e-01]],\n",
       "\n",
       "         [[ 1.3663e+00,  2.7969e-01,  1.6030e+00,  6.8862e-01,  5.9748e-01],\n",
       "          [-1.1997e+00, -2.2862e+00, -9.6298e-01, -1.8773e+00, -1.9685e+00],\n",
       "          [ 5.7863e-01, -5.0796e-01,  8.1530e-01, -9.9033e-02, -1.9017e-01],\n",
       "          [-7.7021e-02, -1.1636e+00,  1.5965e-01, -7.5469e-01, -8.4582e-01],\n",
       "          [ 2.1964e+00,  1.1098e+00,  2.4331e+00,  1.5187e+00,  1.4276e+00],\n",
       "          [-8.0160e-01, -1.8882e+00, -5.6493e-01, -1.4793e+00, -1.5704e+00]],\n",
       "\n",
       "         [[-4.0900e-03,  2.0500e+00,  5.9883e-01,  2.0941e-01,  2.5740e+00],\n",
       "          [-2.5700e+00, -5.1590e-01, -1.9671e+00, -2.3565e+00,  8.0216e-03],\n",
       "          [-7.9174e-01,  1.2624e+00, -1.8882e-01, -5.7824e-01,  1.7863e+00],\n",
       "          [-1.4474e+00,  6.0674e-01, -8.4448e-01, -1.2339e+00,  1.1307e+00],\n",
       "          [ 8.2601e-01,  2.8801e+00,  1.4289e+00,  1.0395e+00,  3.4041e+00],\n",
       "          [-2.1720e+00, -1.1784e-01, -1.5691e+00, -1.9585e+00,  4.0607e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.9766e-01, -1.3291e+00, -4.0755e-01,  6.9152e-01,  8.4104e-01],\n",
       "          [-1.6683e+00, -3.8950e+00, -2.9735e+00, -1.8744e+00, -1.7249e+00],\n",
       "          [ 1.1000e-01, -2.1168e+00, -1.1952e+00, -9.6136e-02,  5.3390e-02],\n",
       "          [-5.4565e-01, -2.7724e+00, -1.8509e+00, -7.5179e-01, -6.0226e-01],\n",
       "          [ 1.7278e+00, -4.9900e-01,  4.2255e-01,  1.5216e+00,  1.6711e+00],\n",
       "          [-1.2702e+00, -3.4970e+00, -2.5754e+00, -1.4764e+00, -1.3268e+00]],\n",
       "\n",
       "         [[ 5.8892e-01,  3.6547e-01,  9.7080e-02, -5.5119e-01,  7.9794e-01],\n",
       "          [-1.9770e+00, -2.2005e+00, -2.4689e+00, -3.1171e+00, -1.7680e+00],\n",
       "          [-1.9873e-01, -4.2219e-01, -6.9057e-01, -1.3388e+00,  1.0288e-02],\n",
       "          [-8.5438e-01, -1.0778e+00, -1.3462e+00, -1.9945e+00, -6.4536e-01],\n",
       "          [ 1.4190e+00,  1.1956e+00,  9.2718e-01,  2.7891e-01,  1.6280e+00],\n",
       "          [-1.5790e+00, -1.8024e+00, -2.0708e+00, -2.7191e+00, -1.3699e+00]],\n",
       "\n",
       "         [[-1.1914e+00,  3.5181e-01,  5.2203e-01,  1.5618e+00,  2.4732e+00],\n",
       "          [-3.7573e+00, -2.2141e+00, -2.0439e+00, -1.0042e+00, -9.2789e-02],\n",
       "          [-1.9790e+00, -4.3585e-01, -2.6563e-01,  7.7411e-01,  1.6855e+00],\n",
       "          [-2.6347e+00, -1.0915e+00, -9.2128e-01,  1.1845e-01,  1.0298e+00],\n",
       "          [-3.6128e-01,  1.1819e+00,  1.3521e+00,  2.3919e+00,  3.3033e+00],\n",
       "          [-3.3593e+00, -1.8161e+00, -1.6459e+00, -6.0613e-01,  3.0526e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.3758e-01, -7.6003e-01, -9.2387e-05,  1.1276e-01,  1.1371e+00],\n",
       "          [-8.7107e-01, -1.9687e+00, -1.2087e+00, -1.0959e+00, -7.1505e-02],\n",
       "          [ 2.7613e-01, -8.2148e-01, -6.1541e-02,  5.1313e-02,  1.0757e+00],\n",
       "          [-8.8887e-01, -1.9865e+00, -1.2265e+00, -1.1137e+00, -8.9298e-02],\n",
       "          [ 1.4820e+00,  3.8443e-01,  1.1444e+00,  1.2572e+00,  2.2816e+00],\n",
       "          [ 1.2668e+00,  1.6920e-01,  9.2914e-01,  1.0420e+00,  2.0664e+00]],\n",
       "\n",
       "         [[ 4.3390e-02, -1.0432e+00,  2.8006e-01, -6.3428e-01, -7.2541e-01],\n",
       "          [-1.1653e+00, -2.2519e+00, -9.2859e-01, -1.8429e+00, -1.9341e+00],\n",
       "          [-1.8059e-02, -1.1047e+00,  2.1861e-01, -6.9572e-01, -7.8686e-01],\n",
       "          [-1.1831e+00, -2.2696e+00, -9.4638e-01, -1.8607e+00, -1.9519e+00],\n",
       "          [ 1.1878e+00,  1.0126e-01,  1.4245e+00,  5.1018e-01,  4.1905e-01],\n",
       "          [ 9.7262e-01, -1.1397e-01,  1.2093e+00,  2.9496e-01,  2.0382e-01]],\n",
       "\n",
       "         [[-1.3270e+00,  7.2715e-01, -7.2407e-01, -1.1135e+00,  1.2511e+00],\n",
       "          [-2.5356e+00, -4.8150e-01, -1.9327e+00, -2.3221e+00,  4.2415e-02],\n",
       "          [-1.3884e+00,  6.6570e-01, -7.8551e-01, -1.1749e+00,  1.1896e+00],\n",
       "          [-2.5534e+00, -4.9930e-01, -1.9505e+00, -2.3399e+00,  2.4622e-02],\n",
       "          [-1.8253e-01,  1.8716e+00,  4.2039e-01,  3.0973e-02,  2.3955e+00],\n",
       "          [-3.9775e-01,  1.6564e+00,  2.0517e-01, -1.8425e-01,  2.1803e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.2524e-01, -2.6520e+00, -1.7304e+00, -6.3138e-01, -4.8185e-01],\n",
       "          [-1.6339e+00, -3.8606e+00, -2.9391e+00, -1.8400e+00, -1.6905e+00],\n",
       "          [-4.8669e-01, -2.7134e+00, -1.7919e+00, -6.9283e-01, -5.4330e-01],\n",
       "          [-1.6517e+00, -3.8784e+00, -2.9569e+00, -1.8578e+00, -1.7083e+00],\n",
       "          [ 7.1922e-01, -1.5075e+00, -5.8598e-01,  5.1308e-01,  6.6261e-01],\n",
       "          [ 5.0399e-01, -1.7228e+00, -8.0121e-01,  2.9786e-01,  4.4738e-01]],\n",
       "\n",
       "         [[-7.3397e-01, -9.5743e-01, -1.2258e+00, -1.8741e+00, -5.2495e-01],\n",
       "          [-1.9426e+00, -2.1661e+00, -2.4345e+00, -3.0827e+00, -1.7336e+00],\n",
       "          [-7.9542e-01, -1.0189e+00, -1.2873e+00, -1.9355e+00, -5.8640e-01],\n",
       "          [-1.9604e+00, -2.1839e+00, -2.4523e+00, -3.1005e+00, -1.7514e+00],\n",
       "          [ 4.1049e-01,  1.8703e-01, -8.1357e-02, -7.2962e-01,  6.1951e-01],\n",
       "          [ 1.9526e-01, -2.8196e-02, -2.9658e-01, -9.4485e-01,  4.0428e-01]],\n",
       "\n",
       "         [[-2.5143e+00, -9.7109e-01, -8.0087e-01,  2.3886e-01,  1.1503e+00],\n",
       "          [-3.7229e+00, -2.1797e+00, -2.0095e+00, -9.6979e-01, -5.8396e-02],\n",
       "          [-2.5757e+00, -1.0325e+00, -8.6232e-01,  1.7742e-01,  1.0888e+00],\n",
       "          [-3.7407e+00, -2.1975e+00, -2.0273e+00, -9.8758e-01, -7.6189e-02],\n",
       "          [-1.3698e+00,  1.7337e-01,  3.4359e-01,  1.3833e+00,  2.2947e+00],\n",
       "          [-1.5850e+00, -4.1858e-02,  1.2836e-01,  1.1681e+00,  2.0795e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 7.7929e-01, -3.1831e-01,  4.4162e-01,  5.5448e-01,  1.5789e+00],\n",
       "          [ 2.8895e+00,  1.7919e+00,  2.5518e+00,  2.6647e+00,  3.6891e+00],\n",
       "          [ 6.2578e-02, -1.0350e+00, -2.7509e-01, -1.6224e-01,  8.6215e-01],\n",
       "          [ 2.5559e+00,  1.4583e+00,  2.2182e+00,  2.3311e+00,  3.3555e+00],\n",
       "          [-2.7897e-01, -1.3766e+00, -6.1664e-01, -5.0378e-01,  5.2060e-01],\n",
       "          [ 4.7295e-01, -6.2466e-01,  1.3528e-01,  2.4813e-01,  1.2725e+00]],\n",
       "\n",
       "         [[ 4.8511e-01, -6.0149e-01,  7.2178e-01, -1.9256e-01, -2.8369e-01],\n",
       "          [ 2.5953e+00,  1.5087e+00,  2.8320e+00,  1.9176e+00,  1.8265e+00],\n",
       "          [-2.3161e-01, -1.3182e+00,  5.0630e-03, -9.0927e-01, -1.0004e+00],\n",
       "          [ 2.2617e+00,  1.1751e+00,  2.4984e+00,  1.5840e+00,  1.4929e+00],\n",
       "          [-5.7315e-01, -1.6597e+00, -3.3648e-01, -1.2508e+00, -1.3420e+00],\n",
       "          [ 1.7876e-01, -9.0784e-01,  4.1543e-01, -4.9891e-01, -5.9004e-01]],\n",
       "\n",
       "         [[-8.8527e-01,  1.1689e+00, -2.8235e-01, -6.7177e-01,  1.6928e+00],\n",
       "          [ 1.2249e+00,  3.2791e+00,  1.8278e+00,  1.4384e+00,  3.8030e+00],\n",
       "          [-1.6020e+00,  4.5215e-01, -9.9907e-01, -1.3885e+00,  9.7607e-01],\n",
       "          [ 8.9133e-01,  2.9455e+00,  1.4942e+00,  1.1048e+00,  3.4694e+00],\n",
       "          [-1.9435e+00,  1.1061e-01, -1.3406e+00, -1.7300e+00,  6.3452e-01],\n",
       "          [-1.1916e+00,  8.6252e-01, -5.8870e-01, -9.7812e-01,  1.3864e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.6477e-02, -2.2103e+00, -1.2887e+00, -1.8966e-01, -4.0135e-02],\n",
       "          [ 2.1267e+00, -1.0009e-01,  8.2146e-01,  1.9205e+00,  2.0701e+00],\n",
       "          [-7.0024e-01, -2.9270e+00, -2.0054e+00, -9.0638e-01, -7.5685e-01],\n",
       "          [ 1.7931e+00, -4.3368e-01,  4.8787e-01,  1.5869e+00,  1.7365e+00],\n",
       "          [-1.0418e+00, -3.2685e+00, -2.3470e+00, -1.2479e+00, -1.0984e+00],\n",
       "          [-2.8987e-01, -2.5166e+00, -1.5951e+00, -4.9601e-01, -3.4648e-01]],\n",
       "\n",
       "         [[-2.9225e-01, -5.1571e-01, -7.8410e-01, -1.4324e+00, -8.3237e-02],\n",
       "          [ 1.8179e+00,  1.5945e+00,  1.3261e+00,  6.7782e-01,  2.0270e+00],\n",
       "          [-1.0090e+00, -1.2324e+00, -1.5008e+00, -2.1491e+00, -7.9995e-01],\n",
       "          [ 1.4843e+00,  1.2609e+00,  9.9250e-01,  3.4423e-01,  1.6934e+00],\n",
       "          [-1.3505e+00, -1.5740e+00, -1.8424e+00, -2.4906e+00, -1.1415e+00],\n",
       "          [-5.9860e-01, -8.2206e-01, -1.0904e+00, -1.7387e+00, -3.8959e-01]],\n",
       "\n",
       "         [[-2.0726e+00, -5.2937e-01, -3.5915e-01,  6.8058e-01,  1.5920e+00],\n",
       "          [ 3.7632e-02,  1.5808e+00,  1.7510e+00,  2.7908e+00,  3.7022e+00],\n",
       "          [-2.7893e+00, -1.2461e+00, -1.0759e+00, -3.6135e-02,  8.7526e-01],\n",
       "          [-2.9596e-01,  1.2472e+00,  1.4174e+00,  2.4572e+00,  3.3686e+00],\n",
       "          [-3.1308e+00, -1.5876e+00, -1.4174e+00, -3.7768e-01,  5.3371e-01],\n",
       "          [-2.3789e+00, -8.3572e-01, -6.6550e-01,  3.7423e-01,  1.2856e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8035e-01, -5.1726e-01,  2.4268e-01,  3.5553e-01,  1.3799e+00],\n",
       "          [ 4.7249e-01, -6.2512e-01,  1.3482e-01,  2.4768e-01,  1.2721e+00],\n",
       "          [ 1.2709e+00,  1.7327e-01,  9.3321e-01,  1.0461e+00,  2.0704e+00],\n",
       "          [ 1.6088e+00,  5.1118e-01,  1.2711e+00,  1.3840e+00,  2.4084e+00],\n",
       "          [ 1.5802e+00,  4.8255e-01,  1.2425e+00,  1.3553e+00,  2.3797e+00],\n",
       "          [-1.3862e-01, -1.2362e+00, -4.7629e-01, -3.6344e-01,  6.6095e-01]],\n",
       "\n",
       "         [[ 2.8616e-01, -8.0043e-01,  5.2283e-01, -3.9150e-01, -4.8264e-01],\n",
       "          [ 1.7831e-01, -9.0829e-01,  4.1498e-01, -4.9936e-01, -5.9050e-01],\n",
       "          [ 9.7669e-01, -1.0990e-01,  1.2134e+00,  2.9902e-01,  2.0789e-01],\n",
       "          [ 1.3146e+00,  2.2801e-01,  1.5513e+00,  6.3694e-01,  5.4580e-01],\n",
       "          [ 1.2860e+00,  1.9938e-01,  1.5226e+00,  6.0831e-01,  5.1717e-01],\n",
       "          [-4.3281e-01, -1.5194e+00, -1.9614e-01, -1.1105e+00, -1.2016e+00]],\n",
       "\n",
       "         [[-1.0842e+00,  9.6992e-01, -4.8129e-01, -8.7072e-01,  1.4938e+00],\n",
       "          [-1.1921e+00,  8.6206e-01, -5.8915e-01, -9.7857e-01,  1.3860e+00],\n",
       "          [-3.9369e-01,  1.6604e+00,  2.0923e-01, -1.8019e-01,  2.1844e+00],\n",
       "          [-5.5773e-02,  1.9984e+00,  5.4715e-01,  1.5773e-01,  2.5223e+00],\n",
       "          [-8.4405e-02,  1.9697e+00,  5.1852e-01,  1.2909e-01,  2.4936e+00],\n",
       "          [-1.8032e+00,  2.5095e-01, -1.2003e+00, -1.5897e+00,  7.7487e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8247e-01, -2.4092e+00, -1.4877e+00, -3.8861e-01, -2.3908e-01],\n",
       "          [-2.9032e-01, -2.5171e+00, -1.5955e+00, -4.9646e-01, -3.4694e-01],\n",
       "          [ 5.0806e-01, -1.7187e+00, -7.9714e-01,  3.0192e-01,  4.5145e-01],\n",
       "          [ 8.4597e-01, -1.3808e+00, -4.5923e-01,  6.3984e-01,  7.8936e-01],\n",
       "          [ 8.1734e-01, -1.4094e+00, -4.8786e-01,  6.1120e-01,  7.6073e-01],\n",
       "          [-9.0144e-01, -3.1282e+00, -2.2066e+00, -1.1076e+00, -9.5805e-01]],\n",
       "\n",
       "         [[-4.9120e-01, -7.1466e-01, -9.8305e-01, -1.6313e+00, -2.8218e-01],\n",
       "          [-5.9906e-01, -8.2251e-01, -1.0909e+00, -1.7392e+00, -3.9004e-01],\n",
       "          [ 1.9933e-01, -2.4129e-02, -2.9252e-01, -9.4078e-01,  4.0835e-01],\n",
       "          [ 5.3724e-01,  3.1378e-01,  4.5397e-02, -6.0287e-01,  7.4626e-01],\n",
       "          [ 5.0861e-01,  2.8515e-01,  1.6764e-02, -6.3150e-01,  7.1763e-01],\n",
       "          [-1.2102e+00, -1.4336e+00, -1.7020e+00, -2.3503e+00, -1.0012e+00]],\n",
       "\n",
       "         [[-2.2715e+00, -7.2832e-01, -5.5810e-01,  4.8164e-01,  1.3930e+00],\n",
       "          [-2.3794e+00, -8.3618e-01, -6.6595e-01,  3.7378e-01,  1.2852e+00],\n",
       "          [-1.5810e+00, -3.7792e-02,  1.3243e-01,  1.1722e+00,  2.0836e+00],\n",
       "          [-1.2431e+00,  3.0012e-01,  4.7034e-01,  1.5101e+00,  2.4215e+00],\n",
       "          [-1.2717e+00,  2.7149e-01,  4.4171e-01,  1.4814e+00,  2.3928e+00],\n",
       "          [-2.9905e+00, -1.4473e+00, -1.2771e+00, -2.3733e-01,  6.7406e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.6880e-01, -1.6664e+00, -9.0647e-01, -7.9362e-01,  2.3077e-01],\n",
       "          [-4.7064e-01, -1.5683e+00, -8.0831e-01, -6.9546e-01,  3.2893e-01],\n",
       "          [ 1.7559e+00,  6.5832e-01,  1.4183e+00,  1.5311e+00,  2.5555e+00],\n",
       "          [ 1.1653e+00,  6.7736e-02,  8.2767e-01,  9.4053e-01,  1.9649e+00],\n",
       "          [ 1.0978e-01, -9.8782e-01, -2.2789e-01, -1.1503e-01,  9.0935e-01],\n",
       "          [ 1.5256e+00,  4.2796e-01,  1.1879e+00,  1.3008e+00,  2.3251e+00]],\n",
       "\n",
       "         [[-8.6299e-01, -1.9496e+00, -6.2632e-01, -1.5407e+00, -1.6318e+00],\n",
       "          [-7.6483e-01, -1.8514e+00, -5.2816e-01, -1.4425e+00, -1.5336e+00],\n",
       "          [ 1.4617e+00,  3.7515e-01,  1.6984e+00,  7.8407e-01,  6.9294e-01],\n",
       "          [ 8.7116e-01, -2.1544e-01,  1.1078e+00,  1.9349e-01,  1.0236e-01],\n",
       "          [-1.8440e-01, -1.2710e+00,  5.2269e-02, -8.6207e-01, -9.5320e-01],\n",
       "          [ 1.2314e+00,  1.4479e-01,  1.4681e+00,  5.5372e-01,  4.6258e-01]],\n",
       "\n",
       "         [[-2.2334e+00, -1.7923e-01, -1.6304e+00, -2.0199e+00,  3.4468e-01],\n",
       "          [-2.1352e+00, -8.1071e-02, -1.5323e+00, -1.9217e+00,  4.4285e-01],\n",
       "          [ 9.1362e-02,  2.1455e+00,  6.9428e-01,  3.0486e-01,  2.6694e+00],\n",
       "          [-4.9922e-01,  1.5549e+00,  1.0370e-01, -2.8572e-01,  2.0788e+00],\n",
       "          [-1.5548e+00,  4.9936e-01, -9.5186e-01, -1.3413e+00,  1.0233e+00],\n",
       "          [-1.3899e-01,  1.9151e+00,  4.6393e-01,  7.4507e-02,  2.4391e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3316e+00, -3.5584e+00, -2.6368e+00, -1.5378e+00, -1.3882e+00],\n",
       "          [-1.2335e+00, -3.4602e+00, -2.5387e+00, -1.4396e+00, -1.2901e+00],\n",
       "          [ 9.9311e-01, -1.2336e+00, -3.1209e-01,  7.8697e-01,  9.3650e-01],\n",
       "          [ 4.0253e-01, -1.8242e+00, -9.0268e-01,  1.9639e-01,  3.4591e-01],\n",
       "          [-6.5303e-01, -2.8798e+00, -1.9582e+00, -8.5917e-01, -7.0965e-01],\n",
       "          [ 7.6275e-01, -1.4640e+00, -5.4245e-01,  5.5662e-01,  7.0614e-01]],\n",
       "\n",
       "         [[-1.6404e+00, -1.8638e+00, -2.1322e+00, -2.7805e+00, -1.4313e+00],\n",
       "          [-1.5422e+00, -1.7656e+00, -2.0340e+00, -2.6823e+00, -1.3332e+00],\n",
       "          [ 6.8438e-01,  4.6092e-01,  1.9253e-01, -4.5573e-01,  8.9339e-01],\n",
       "          [ 9.3795e-02, -1.2966e-01, -3.9805e-01, -1.0463e+00,  3.0281e-01],\n",
       "          [-9.6177e-01, -1.1852e+00, -1.4536e+00, -2.1019e+00, -7.5275e-01],\n",
       "          [ 4.5402e-01,  2.3056e-01, -3.7824e-02, -6.8609e-01,  6.6304e-01]],\n",
       "\n",
       "         [[-3.4207e+00, -1.8775e+00, -1.7073e+00, -6.6752e-01,  2.4387e-01],\n",
       "          [-3.3225e+00, -1.7793e+00, -1.6091e+00, -5.6936e-01,  3.4204e-01],\n",
       "          [-1.0959e+00,  4.4726e-01,  6.1748e-01,  1.6572e+00,  2.5686e+00],\n",
       "          [-1.6865e+00, -1.4332e-01,  2.6897e-02,  1.0666e+00,  1.9780e+00],\n",
       "          [-2.7421e+00, -1.1989e+00, -1.0287e+00,  1.1071e-02,  9.2246e-01],\n",
       "          [-1.3263e+00,  2.1690e-01,  3.8712e-01,  1.4269e+00,  2.3382e+00]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-dimensional broadcasting\n",
    "a = torch.randn(8, 1, 6, 1)      # shape: (8, 1, 6, 1)\n",
    "b = torch.randn(7, 1, 5)         # shape: (7, 1, 5)\n",
    "\n",
    "# Broadcasting rules applied:\n",
    "# a: (8, 1, 6, 1) -> (8, 7, 6, 5)\n",
    "# b: (   7, 1, 5) -> (8, 7, 6, 5)\n",
    "result = a + b                    # final shape: (8, 7, 6, 5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0974e",
   "metadata": {},
   "source": [
    "> ![](https://av-eks-blogoptimized.s3.amazonaws.com/15357brd_fig_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab1c3b",
   "metadata": {},
   "source": [
    "#### **Practical Benefits:**\n",
    "\n",
    "**Broadcasting provides several advantages:**\n",
    "\n",
    "1. **`Memory Efficiency`**: Avoids creating unnecessary copies of data by working with views when possible.\n",
    "\n",
    "2. **`Code Simplicity`**: Eliminates the need for explicit loops or manual reshaping in many cases.\n",
    "\n",
    "3. **`Performance`**: Optimized implementations can perform broadcasted operations faster than equivalent manual operations.\n",
    "\n",
    "4. **`Flexibility`**: Allows natural mathematical operations between arrays of different but compatible shapes.\n",
    "\n",
    "The key to mastering broadcasting is understanding how PyTorch aligns dimensions and applies the compatibility rules, which enables writing more concise and efficient tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2a5a5",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "------\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785c0a9",
   "metadata": {},
   "source": [
    "## **Outer Product:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65f953",
   "metadata": {},
   "source": [
    "The outer product is a fundamental operation in linear algebra that takes two vectors and produces a matrix. \n",
    "\n",
    "Given two vectors **$u$** and **$v$**, their outer product creates a matrix where each element is the product of corresponding elements from the two vectors.\n",
    "\n",
    "For vectors **$u$** $∈ ℝᵐ$ and **v** $∈ ℝⁿ$, the outer product **$u$** ⊗ **v** (or **uv**ᵀ) produces an $m×n$ matrix **$M$** where:\n",
    "\n",
    "**$M$**[i,j] = **$u$**[i] × **$v$**[j]\n",
    "\n",
    "This is equivalent to multiplying a column vector by a row vector: **$u$** $(m×1)$ × **v**ᵀ $(1×n)$ = **$M$** $(m×n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c586f0",
   "metadata": {},
   "source": [
    "> ![](https://media.geeksforgeeks.org/wp-content/uploads/20190413155438/outerProduct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954c0ab",
   "metadata": {},
   "source": [
    "**Geometric Interpretation:**\n",
    "\n",
    "The outer product captures the interaction between every pair of components from two vectors. Each row of the resulting matrix is the first vector scaled by the corresponding element of the second vector.\n",
    "\n",
    "PyTorch provides several ways to compute outer products:\n",
    "\n",
    "**Method 1: Using `torch.outer()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: tensor([1., 2., 3.])\n",
      "V: tensor([4., 5., 6., 7.])\n",
      "\n",
      "Outer Product:\n",
      " tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [12., 15., 18., 21.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define two vectors\n",
    "u = torch.tensor([1., 2., 3.])      # shape: (3,)\n",
    "v = torch.tensor([4., 5., 6., 7.])  # shape: (4,)\n",
    "print(f\"U: {u}\")\n",
    "print(f\"V: {v}\")\n",
    "\n",
    "# Compute outer product\n",
    "outer_product = torch.outer(u, v)   # shape: (3, 4)\n",
    "print(f\"\\nOuter Product:\\n {outer_product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a542f59",
   "metadata": {},
   "source": [
    "**Method 2: Using Matrix Multiplication:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0531b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U col:\n",
      " tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "\n",
      "V row:\n",
      " tensor([[4., 5., 6., 7.]])\n",
      "\n",
      "Outer Product:\n",
      " tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [12., 15., 18., 21.]])\n"
     ]
    }
   ],
   "source": [
    "# Reshape vectors for matrix multiplication\n",
    "u_col = u.unsqueeze(1)    # shape: (3, 1) - column vector\n",
    "v_row = v.unsqueeze(0)    # shape: (1, 4) - row vector\n",
    "print(f\"U col:\\n {u_col}\\n\")\n",
    "print(f\"V row:\\n {v_row}\\n\")\n",
    "outer_product = u_col @ v_row  # or torch.mm(u_col, v_row)\n",
    "print(f\"Outer Product:\\n {outer_product}\")\n",
    "# Same result as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804e4a9",
   "metadata": {},
   "source": [
    "**Method 3: Using Broadcasting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27e9833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [12., 15., 18., 21.]])\n"
     ]
    }
   ],
   "source": [
    "# Leverage broadcasting for outer product\n",
    "u_expanded = u.unsqueeze(1)  # shape: (3, 1)\n",
    "v_expanded = v.unsqueeze(0)  # shape: (1, 4)\n",
    "\n",
    "outer_product = u_expanded * v_expanded  # Broadcasting creates (3, 4)\n",
    "print(outer_product)\n",
    "# Same result as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a0bec8",
   "metadata": {},
   "source": [
    "**Method 4: Using `torch.einsum()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af77a68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [12., 15., 18., 21.]])\n"
     ]
    }
   ],
   "source": [
    "# Einstein summation notation\n",
    "outer_product = torch.einsum('i,j->ij', u, v)\n",
    "print(outer_product)\n",
    "# Same result as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f5378",
   "metadata": {},
   "source": [
    "### **Practical Examples:**\n",
    "\n",
    "**Example 1: Feature Interaction Matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b7f29d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Creating interaction features between two feature vectors\n",
    "features_a = torch.tensor([0.5, 1.2, 0.8])     # 3 features\n",
    "features_b = torch.tensor([2.0, 1.5, 0.9, 1.1]) # 4 features\n",
    "\n",
    "# Outer product creates all pairwise interactions\n",
    "interaction_matrix = torch.outer(features_a, features_b)\n",
    "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "# Shape: (3, 4) - captures all 12 possible feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe23ed52",
   "metadata": {},
   "source": [
    "**Example 2: Batch Outer Products:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "242fe4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch outer product shape: torch.Size([32, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Computing outer products for batches of vectors\n",
    "batch_u = torch.randn(32, 5)  # 32 samples, 5-dimensional vectors\n",
    "batch_v = torch.randn(32, 3)  # 32 samples, 3-dimensional vectors\n",
    "\n",
    "# Method 1: Using torch.bmm (batch matrix multiplication)\n",
    "u_batch = batch_u.unsqueeze(2)  # shape: (32, 5, 1)\n",
    "v_batch = batch_v.unsqueeze(1)  # shape: (32, 1, 3)\n",
    "batch_outer = torch.bmm(u_batch, v_batch)  # shape: (32, 5, 3)\n",
    "\n",
    "# Method 2: Using einsum for batch outer products\n",
    "batch_outer_einsum = torch.einsum('bi,bj->bij', batch_u, batch_v)\n",
    "print(f\"Batch outer product shape: {batch_outer_einsum.shape}\")\n",
    "# Shape: (32, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4bb35",
   "metadata": {},
   "source": [
    "**Example 3: Rank-1 Matrix Decomposition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ed4eb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix rank: 1\n"
     ]
    }
   ],
   "source": [
    "# Creating a rank-1 matrix (outer product creates matrices of rank 1)\n",
    "u = torch.tensor([1., 2., 3.])\n",
    "v = torch.tensor([4., 5.])\n",
    "\n",
    "rank_1_matrix = torch.outer(u, v)\n",
    "print(f\"Matrix rank: {torch.linalg.matrix_rank(rank_1_matrix)}\")\n",
    "# Output: Matrix rank: 1\n",
    "\n",
    "# Any rank-1 matrix can be decomposed as an outer product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff312003",
   "metadata": {},
   "source": [
    "**Example 4: Gradient Computation in Neural Networks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0ba22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight gradient shape: torch.Size([10, 8])\n"
     ]
    }
   ],
   "source": [
    "# Outer products appear in gradient computations\n",
    "# For example, in computing gradients of bilinear layers\n",
    "\n",
    "input_vector = torch.randn(10, requires_grad=True)\n",
    "output_vector = torch.randn(8, requires_grad=True)\n",
    "\n",
    "# Bilinear operation simulation\n",
    "weight_gradient = torch.outer(input_vector, output_vector)\n",
    "print(f\"Weight gradient shape: {weight_gradient.shape}\")\n",
    "# Shape: (10, 8) - represents gradient of a bilinear weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ba468",
   "metadata": {},
   "source": [
    "**Memory Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec9d7e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 3.81 MB\n"
     ]
    }
   ],
   "source": [
    "# Outer products can create large matrices\n",
    "u = torch.randn(1000)\n",
    "v = torch.randn(1000)\n",
    "\n",
    "# This creates a 1000×1000 matrix (4MB for float32)\n",
    "large_outer = torch.outer(u, v)\n",
    "print(f\"Memory usage: {large_outer.numel() * 4 / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cebb54",
   "metadata": {},
   "source": [
    "**Computational Complexity:**\n",
    "The outer product has O(mn) complexity where m and n are the lengths of the input vectors, as it computes m×n products.\n",
    "\n",
    "#### **Common Use Cases:**\n",
    "\n",
    "1. **Machine Learning**: Feature interactions, attention mechanisms, and bilinear transformations.\n",
    "\n",
    "2. **Computer Vision**: Constructing covariance matrices and kernel matrices.\n",
    "\n",
    "3. **Signal Processing**: Creating correlation matrices and spectrograms.\n",
    "\n",
    "4. **Graph Theory**: Adjacency matrix construction from node features.\n",
    "\n",
    "The outer product is a powerful operation that bridges vector operations with matrix computations, making it essential for many advanced linear algebra applications in deep learning and scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46457b",
   "metadata": {},
   "source": [
    "----\n",
    "-----\n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0adad8",
   "metadata": {},
   "source": [
    "## **Inner Product:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa61da7",
   "metadata": {},
   "source": [
    "The `inner product` (also called `dot product` or `scalar product`) is a fundamental operation in linear algebra that takes two vectors and produces a scalar value. \n",
    "\n",
    "It measures the `similarity` between vectors and captures their geometric relationship.\n",
    "\n",
    "For two vectors **$u$** = $[u₁, u₂, ..., uₙ]$ and **$v$** = $[v₁, v₂, ..., vₙ]$ in ℝⁿ, the inner product is:\n",
    "\n",
    "> **$u$** · **$v$** = $u₁v₁ + u₂v₂ + ... + uₙvₙ = Σᵢ uᵢvᵢ$\n",
    "\n",
    "Geometrically, it equals: **$u$** · **$v$** = ||**$u$**|| ||**$v$**|| $cos(θ)$, where $θ$ is the angle between the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a6d00",
   "metadata": {},
   "source": [
    "#### **Properties and Interpretation:**\n",
    "\n",
    "1. **`Similarity Measure`**: Higher values indicate more similar vectors (same direction).\n",
    "\n",
    "2. **`Orthogonality`**: Inner product = 0 means vectors are perpendicular.\n",
    "\n",
    "3. **`Magnitude Relationship`**: The inner product relates to vector lengths and angles.\n",
    "\n",
    "4. **`Projection`**: **$u$** · **$v$** / ||**$v$**|| gives the projection of **$u$** onto **$v$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2b29d",
   "metadata": {},
   "source": [
    "#### **Inner Product in PyTorch**\n",
    "\n",
    "**Method 1: Using `torch.dot()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "490eadaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# For 1D vectors only\n",
    "u = torch.tensor([1., 2., 3.])\n",
    "v = torch.tensor([4., 5., 6.])\n",
    "\n",
    "inner_product = torch.dot(u, v)\n",
    "print(inner_product)  # Output: tensor(32.) = 1*4 + 2*5 + 3*6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04f732",
   "metadata": {},
   "source": [
    "**Method 2: Using `torch.matmul()` or `@`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f277502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "# Works for various tensor shapes\n",
    "u = torch.tensor([1., 2., 3.])\n",
    "v = torch.tensor([4., 5., 6.])\n",
    "\n",
    "inner_product = torch.matmul(u, v)  # or u @ v\n",
    "print(inner_product)  # Output: tensor(32.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67c051",
   "metadata": {},
   "source": [
    "**Method 3: Using Element-wise Multiplication and Sum:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "269f1896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "# Manual computation\n",
    "u = torch.tensor([1., 2., 3.])\n",
    "v = torch.tensor([4., 5., 6.])\n",
    "\n",
    "inner_product = torch.sum(u * v)\n",
    "print(inner_product)  # Output: tensor(32.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ddb13",
   "metadata": {},
   "source": [
    "**Method 4: Using `torch.einsum()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47e92b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "# Einstein summation notation\n",
    "inner_product = torch.einsum('i,i->', u, v)\n",
    "print(inner_product)  # Output: tensor(32.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe668a4",
   "metadata": {},
   "source": [
    "#### **Batch Inner Products:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0572cf",
   "metadata": {},
   "source": [
    "\n",
    "### Computing Multiple Inner Products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82491368",
   "metadata": {},
   "source": [
    "```python\n",
    "# Batch of vectors\n",
    "batch_u = torch.randn(32, 128)  # 32 vectors of dimension 128\n",
    "batch_v = torch.randn(32, 128)  # 32 vectors of dimension 128\n",
    "\n",
    "# Element-wise inner products\n",
    "batch_inner = torch.sum(batch_u * batch_v, dim=1)  # shape: (32,)\n",
    "\n",
    "# Or using einsum\n",
    "batch_inner_einsum = torch.einsum('bi,bi->b', batch_u, batch_v)\n",
    "print(f\"Batch inner products shape: {batch_inner.shape}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73150ff",
   "metadata": {},
   "source": [
    "\n",
    "### Matrix-Vector Inner Products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f4d71",
   "metadata": {},
   "source": [
    "```python\n",
    "# Inner product between each row of matrix and a vector\n",
    "matrix = torch.randn(10, 5)  # 10 vectors of dimension 5\n",
    "vector = torch.randn(5)      # single vector\n",
    "\n",
    "# Each row's inner product with vector\n",
    "inner_products = matrix @ vector  # shape: (10,)\n",
    "print(f\"Inner products shape: {inner_products.shape}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e1c6c",
   "metadata": {},
   "source": [
    "\n",
    "## Uses in Vector Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909ea1b",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Similarity Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43775d",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vector database similarity search simulation\n",
    "def cosine_similarity(query_vector, database_vectors):\n",
    "    \"\"\"Compute cosine similarity using inner products\"\"\"\n",
    "    # Normalize vectors\n",
    "    query_norm = query_vector / torch.norm(query_vector)\n",
    "    db_norm = database_vectors / torch.norm(database_vectors, dim=1, keepdim=True)\n",
    "    \n",
    "    # Cosine similarity = normalized inner product\n",
    "    similarities = db_norm @ query_norm\n",
    "    return similarities\n",
    "\n",
    "# Example usage\n",
    "query = torch.randn(512)        # Query embedding\n",
    "database = torch.randn(1000, 512)  # Database embeddings\n",
    "\n",
    "similarities = cosine_similarity(query, database)\n",
    "top_k_indices = torch.topk(similarities, k=5).indices\n",
    "print(f\"Top 5 similar vectors: {top_k_indices}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3edae",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Approximate Nearest Neighbor Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0485944",
   "metadata": {},
   "source": [
    "```python\n",
    "# Efficient similarity computation for large databases\n",
    "def efficient_search(query_embedding, database_embeddings, top_k=10):\n",
    "    \"\"\"Efficient vector similarity search\"\"\"\n",
    "    # Batch inner product computation\n",
    "    similarities = database_embeddings @ query_embedding\n",
    "    \n",
    "    # Get top-k most similar\n",
    "    top_similarities, top_indices = torch.topk(similarities, k=top_k)\n",
    "    \n",
    "    return top_indices, top_similarities\n",
    "\n",
    "# Example with large database\n",
    "large_db = torch.randn(100000, 768)  # 100k vectors, 768 dimensions\n",
    "query = torch.randn(768)\n",
    "\n",
    "indices, scores = efficient_search(query, large_db)\n",
    "print(f\"Found {len(indices)} similar vectors\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d793b0d",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Semantic Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b132bd6",
   "metadata": {},
   "source": [
    "```python\n",
    "# Semantic search using pre-computed embeddings\n",
    "class SemanticSearch:\n",
    "    def __init__(self, document_embeddings):\n",
    "        self.embeddings = document_embeddings\n",
    "        # Normalize for cosine similarity\n",
    "        self.normalized_embeddings = self.embeddings / torch.norm(\n",
    "            self.embeddings, dim=1, keepdim=True\n",
    "        )\n",
    "    \n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        # Normalize query\n",
    "        query_norm = query_embedding / torch.norm(query_embedding)\n",
    "        \n",
    "        # Compute similarities using inner product\n",
    "        similarities = self.normalized_embeddings @ query_norm\n",
    "        \n",
    "        # Return top results\n",
    "        return torch.topk(similarities, k=top_k)\n",
    "\n",
    "# Usage example\n",
    "doc_embeddings = torch.randn(10000, 384)  # Document embeddings\n",
    "search_engine = SemanticSearch(doc_embeddings)\n",
    "\n",
    "query_emb = torch.randn(384)\n",
    "results = search_engine.search(query_emb, top_k=3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0da362",
   "metadata": {},
   "source": [
    "\n",
    "## Uses in Deep Learning\n",
    "\n",
    "### 1. Attention Mechanisms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6398b67",
   "metadata": {},
   "source": [
    "```python\n",
    "# Scaled dot-product attention\n",
    "def scaled_dot_product_attention(query, key, value, scale=None):\n",
    "    \"\"\"Attention mechanism using inner products\"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scale = scale or (1.0 / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)))\n",
    "    \n",
    "    # Inner product between queries and keys\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) * scale\n",
    "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum using inner products\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "seq_len, d_model = 10, 64\n",
    "query = torch.randn(1, seq_len, d_model)\n",
    "key = torch.randn(1, seq_len, d_model)\n",
    "value = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "attention_output = scaled_dot_product_attention(query, key, value)\n",
    "print(f\"Attention output shape: {attention_output.shape}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f8123",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373bbd89",
   "metadata": {},
   "source": [
    "```python\n",
    "# Cosine similarity loss\n",
    "def cosine_similarity_loss(embeddings1, embeddings2, targets):\n",
    "    \"\"\"Cosine similarity loss using inner products\"\"\"\n",
    "    # Normalize embeddings\n",
    "    emb1_norm = embeddings1 / torch.norm(embeddings1, dim=1, keepdim=True)\n",
    "    emb2_norm = embeddings2 / torch.norm(embeddings2, dim=1, keepdim=True)\n",
    "    \n",
    "    # Cosine similarity via inner product\n",
    "    cosine_sim = torch.sum(emb1_norm * emb2_norm, dim=1)\n",
    "    \n",
    "    # Convert to loss\n",
    "    loss = torch.mean((cosine_sim - targets) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Example\n",
    "emb1 = torch.randn(32, 128)  # Batch of embeddings\n",
    "emb2 = torch.randn(32, 128)  # Batch of embeddings\n",
    "targets = torch.ones(32)     # Target similarities\n",
    "\n",
    "loss = cosine_similarity_loss(emb1, emb2, targets)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af27cca",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Neural Network Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd985ad",
   "metadata": {},
   "source": [
    "```python\n",
    "# Linear layer implementation using inner products\n",
    "class CustomLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Each output is inner product of input with weight row\n",
    "        return torch.matmul(x, self.weight.t()) + self.bias\n",
    "\n",
    "# Usage\n",
    "layer = CustomLinear(10, 5)\n",
    "input_tensor = torch.randn(32, 10)\n",
    "output = layer(input_tensor)\n",
    "print(f\"Output shape: {output.shape}\")  # (32, 5)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3eefa7",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Similarity Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40289b0",
   "metadata": {},
   "source": [
    "```python\n",
    "# Triplet loss using inner products\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    \"\"\"Triplet loss using inner product-based distances\"\"\"\n",
    "    # Compute similarities using inner products\n",
    "    pos_sim = torch.sum(anchor * positive, dim=1)\n",
    "    neg_sim = torch.sum(anchor * negative, dim=1)\n",
    "    \n",
    "    # Triplet loss\n",
    "    loss = torch.clamp(neg_sim - pos_sim + margin, min=0.0)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# Example\n",
    "anchor = torch.randn(32, 128)\n",
    "positive = torch.randn(32, 128)\n",
    "negative = torch.randn(32, 128)\n",
    "\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3544a5a",
   "metadata": {},
   "source": [
    "\n",
    "## Performance Optimization\n",
    "\n",
    "### GPU Acceleration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c6eef",
   "metadata": {},
   "source": [
    "```python\n",
    "# Move to GPU for faster computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "large_matrix = torch.randn(10000, 1024).to(device)\n",
    "query_vector = torch.randn(1024).to(device)\n",
    "\n",
    "# Fast inner product computation on GPU\n",
    "similarities = large_matrix @ query_vector\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3515d6",
   "metadata": {},
   "source": [
    "\n",
    "### **Memory-Efficient Batch Processing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c5bac1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28.1641, -4.2277,  0.1349,  ..., -7.6924,  6.2356, 23.3391])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunked_inner_product(matrix, vector, chunk_size=1000):\n",
    "    \"\"\"Compute inner products in chunks to save memory\"\"\"\n",
    "    results = []\n",
    "    for i in range(0, matrix.size(0), chunk_size):\n",
    "        chunk = matrix[i:i+chunk_size]\n",
    "        chunk_result = chunk @ vector\n",
    "        results.append(chunk_result)\n",
    "    return torch.cat(results)\n",
    "\n",
    "# Usage for very large matrices\n",
    "huge_matrix = torch.randn(1000000, 512)\n",
    "vector = torch.randn(512)\n",
    "results = chunked_inner_product(huge_matrix, vector)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cdcd14d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e3e09",
   "metadata": {},
   "source": [
    "The inner product is fundamental to modern AI systems, serving as the core operation in `similarity search`, `attention mechanisms`, and `neural network computations`. Its efficiency and mathematical properties make it indispensable for both `vector databases` and `deep learning applications.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422abe6",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "----\n",
    "-----\n",
    "-----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4adee",
   "metadata": {},
   "source": [
    "## **Vector Magnitude (Norm):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61f4dc",
   "metadata": {},
   "source": [
    "A vector norm is a function that assigns a non-negative real number to each vector in a vector space, representing the `\"size\"` or `\"length\"` of that vector. It's denoted as ||$v$|| for a vector $v$.\n",
    "\n",
    "**Properties of Vector Norms:**\n",
    "\n",
    "Vector norms must satisfy four fundamental properties:\n",
    "\n",
    "1. **`Non-negativity`**: $||v|| ≥ 0$ for all vectors v, and $||v|| = 0$ if and only if $v = 0$\n",
    "\n",
    "2. **`Homogeneity`**: |$|αv|| = |α| ||v||$ for any scalar α and vector $v$\n",
    "\n",
    "3. **`Triangle inequality`**: $||u + v|| ≤ ||u|| + ||v||$ for any vectors $u$ and $v$\n",
    "\n",
    "4. **Defini`teness**: $||v|| = 0$ implies $v = 0$ (zero vector)\n",
    "\n",
    "**Intuitive Meaning:**  \n",
    "Think of a vector norm as measuring the \"distance\" from the origin to the point represented by the vector. In 2D or 3D space, this corresponds to the geometric length of the vector. The norm gives you a single number that captures how \"big\" or \"far\" the vector is, regardless of its direction.\n",
    "\n",
    "For example, if you have a vector representing velocity, its norm tells you the speed (magnitude) without caring about the direction of movement.\n",
    "\n",
    "**Common Types of Norms:**\n",
    "\n",
    "  - **L1 norm (Manhattan norm)**: $||v||₁ = Σ|vᵢ|$ - sum of absolute values\n",
    "  - **L2 norm (Euclidean norm)**: $||v||₂ = √(Σvᵢ²)$ - square root of sum of squares\n",
    "  - **L∞ norm (Maximum norm)**: $||v||∞ = max|vᵢ|$ - maximum absolute value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5642524",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "- **L0 norm**: $||v||₀$ = number of non-zero elements (not technically a norm, but useful for sparsity)\n",
    "\n",
    "- **L1 norm (Manhattan/Taxicab norm)**: $||v||₁ = Σ|vᵢ|$ - sum of absolute values of components\n",
    "\n",
    "- **L2 norm (Euclidean norm)**: $||v||₂ = √(Σvᵢ²)$ - square root of sum of squared components\n",
    "\n",
    "- **Lp norm**: $||v||p = (Σ|vᵢ|ᵖ)^(1/p)$ - generalized p-norm for any $p ≥ 1$\n",
    "\n",
    "- **L∞ norm (Maximum/Chebyshev norm)**: $||v||∞ = max|vᵢ|$ - maximum absolute value among components\n",
    "\n",
    "- **Frobenius norm**: $||A||F = √(Σᵢⱼ|aᵢⱼ|²)$ - extension of L2 norm for matrices\n",
    "\n",
    "- **Spectral norm**: $||A||₂$ = largest singular value of matrix A\n",
    "\n",
    "- **Nuclear norm**: $||A||*$ = sum of singular values of matrix A\n",
    "\n",
    "- **Weighted norm**: $||v||W = √(vᵀWv)$ - norm with positive definite weight matrix W\n",
    "\n",
    "- **Hamming norm**: number of positions where vectors differ (for binary vectors)\n",
    "\n",
    "- **Minkowski norm**: $||v||p = (Σ|vᵢ|ᵖ)^(1/p)$ - another name for Lp norm\n",
    "\n",
    "- **Mahalanobis norm**: $||v||M = √(vᵀM⁻¹v)$ - norm accounting for covariance structure\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7477476",
   "metadata": {},
   "source": [
    "#### **Applications in Deep Learning:**\n",
    "\n",
    "Vector norms are crucial in deep learning for several reasons:\n",
    "\n",
    "1. **`Gradient clipping`**: Prevents exploding gradients by scaling them when their norm exceeds a threshold, ensuring stable training\n",
    "\n",
    "2. **`Regularization`**: L1 and L2 regularization add norm penalties to loss functions, preventing overfitting by encouraging smaller weights\n",
    "\n",
    "3. **`Normalization techniques`**: Batch normalization and layer normalization use norms to standardize inputs, improving training stability and convergence\n",
    "\n",
    "4. **`Weight initialization`**: Proper initialization often involves controlling the norm of initial weights to maintain appropriate signal propagation\n",
    "\n",
    "5. **`Optimization`**: Many optimization algorithms use norms to determine step sizes and convergence criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265cb93",
   "metadata": {},
   "source": [
    "### **Applications in Vector Databases:**\n",
    "\n",
    "Vector databases rely heavily on norms for similarity search and retrieval:\n",
    "\n",
    "1. **`Distance metrics`**: Norms define distance functions (like `Euclidean distance`) used to measure similarity between vectors representing documents, images, or other data\n",
    "\n",
    "2. **`Indexing efficiency`**: Many indexing structures (like $LSH$ or $HNSW$) use norm-based distances to organize and search through high-dimensional vector spaces\n",
    "\n",
    "3. **`Normalization`**: Vectors are often normalized (divided by their norm) to unit length, making cosine similarity equivalent to dot product and improving search performance\n",
    "\n",
    "4. **`Query optimization`**: Norms help prune search spaces by eliminating vectors that are too far from query vectors based on triangle inequality properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ad04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector v1: tensor([3., 4., 5.])\n",
      "Matrix v2: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create example vectors\n",
    "v1 = torch.tensor([3.0, 4.0, 5.0])\n",
    "v2 = torch.tensor([[1.0, 2.0, 3.0], \n",
    "                   [4.0, 5.0, 6.0]])\n",
    "\n",
    "print(\"Vector v1:\", v1)\n",
    "print(\"Matrix v2:\", v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c4d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of v1: 7.071067810058594\n"
     ]
    }
   ],
   "source": [
    "# L2 norm (Euclidean norm) - default\n",
    "l2_norm_v1 = torch.norm(v1)\n",
    "print(f\"L2 norm of v1: {l2_norm_v1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5245a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm (explicit): 7.071067810058594\n"
     ]
    }
   ],
   "source": [
    "# L2 norm with explicit parameter\n",
    "l2_norm_explicit = torch.norm(v1, p=2)\n",
    "print(f\"L2 norm (explicit): {l2_norm_explicit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80cc5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm of v1: 12.0\n"
     ]
    }
   ],
   "source": [
    "# L1 norm (Manhattan norm)\n",
    "l1_norm = torch.norm(v1, p=1)\n",
    "print(f\"L1 norm of v1: {l1_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9918a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L-infinity norm of v1: 5.0\n"
     ]
    }
   ],
   "source": [
    "# L-infinity norm (Maximum norm)\n",
    "linf_norm = torch.norm(v1, p=float('inf'))\n",
    "print(f\"L-infinity norm of v1: {linf_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae55904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm of v2: 9.539392471313477\n"
     ]
    }
   ],
   "source": [
    "# Frobenius norm for matrices (equivalent to L2 for flattened matrix)\n",
    "frobenius_norm = torch.norm(v2)\n",
    "print(f\"Frobenius norm of v2: {frobenius_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8fc8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Norms along specific dimensions ---\n",
      "L2 norm along dim=1: tensor([3.7417, 8.7750])\n",
      "L2 norm along dim=0: tensor([4.1231, 5.3852, 6.7082])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Norms along specific dimensions ---\")\n",
    "\n",
    "# L2 norm along dimension 1 (columns)\n",
    "l2_dim1 = torch.norm(v2, dim=1)\n",
    "print(f\"L2 norm along dim=1: {l2_dim1}\")\n",
    "\n",
    "# L2 norm along dimension 0 (rows)\n",
    "l2_dim0 = torch.norm(v2, dim=0)\n",
    "print(f\"L2 norm along dim=0: {l2_dim0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51574930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Normalization (unit vectors) ---\n",
      "Normalized v1: tensor([0.4243, 0.5657, 0.7071])\n",
      "Norm of normalized v1: 1.0\n",
      "Row-normalized v2:\n",
      "tensor([[0.2673, 0.5345, 0.8018],\n",
      "        [0.4558, 0.5698, 0.6838]])\n",
      "Row norms after normalization: tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Normalization (unit vectors) ---\")\n",
    "\n",
    "# Normalize vector to unit length (L2 norm = 1)\n",
    "v1_normalized = F.normalize(v1, p=2, dim=0)\n",
    "print(f\"Normalized v1: {v1_normalized}\")\n",
    "print(f\"Norm of normalized v1: {torch.norm(v1_normalized)}\")\n",
    "\n",
    "# Normalize matrix rows to unit length\n",
    "v2_normalized = F.normalize(v2, p=2, dim=1)\n",
    "print(f\"Row-normalized v2:\\n{v2_normalized}\")\n",
    "print(f\"Row norms after normalization: {torch.norm(v2_normalized, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8acbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Practical examples ---\n",
      "Original gradient norm: 37.41657257080078\n",
      "Clipped gradients: tensor([1.3363, 2.6726, 4.0089])\n",
      "Clipped gradient norm: 5.0\n",
      "L2 regularization term: 0.06499999761581421\n",
      "Euclidean distance: 5.196152210235596\n",
      "Cosine similarity: 0.9746317863464355\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Practical examples ---\")\n",
    "\n",
    "# Gradient clipping example\n",
    "gradients = torch.tensor([10.0, 20.0, 30.0])\n",
    "max_norm = 5.0\n",
    "grad_norm = torch.norm(gradients)\n",
    "print(f\"Original gradient norm: {grad_norm}\")\n",
    "\n",
    "if grad_norm > max_norm:\n",
    "    clipped_gradients = gradients * (max_norm / grad_norm)\n",
    "    print(f\"Clipped gradients: {clipped_gradients}\")\n",
    "    print(f\"Clipped gradient norm: {torch.norm(clipped_gradients)}\")\n",
    "\n",
    "# L2 regularization term\n",
    "weights = torch.tensor([1.5, -2.0, 0.5])\n",
    "l2_reg = 0.01 * torch.norm(weights, p=2) ** 2\n",
    "print(f\"L2 regularization term: {l2_reg}\")\n",
    "\n",
    "# Distance calculation between vectors\n",
    "point1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "point2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "euclidean_distance = torch.norm(point1 - point2)\n",
    "print(f\"Euclidean distance: {euclidean_distance}\")\n",
    "\n",
    "# Cosine similarity using normalized vectors\n",
    "cos_sim = torch.dot(F.normalize(point1, dim= 0), F.normalize(point2, dim=0), )\n",
    "print(f\"Cosine similarity: {cos_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88281a6e",
   "metadata": {},
   "source": [
    "**PyTorch provides several convenient functions for calculating norms:**\n",
    "\n",
    "- **`torch.norm()`**: Main function for computing various `norms`, with parameters for norm type (`p`) and dimension (`dim`)\n",
    "\n",
    "- **`torch.nn.functional.normalize()`**: Normalizes vectors to unit length, useful for creating unit vectors\n",
    "\n",
    "- **`Gradient clipping`**: Use `torch.nn.utils.clip_grad_norm_()` to clip gradients based on their norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed73fb4",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "-----\n",
    "----\n",
    "-----\n",
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7bfef",
   "metadata": {},
   "source": [
    "## **Unit Vector (Normalization):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afb6a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d47d6b56",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----\n",
    "-----\n",
    "-----\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a10d4f",
   "metadata": {},
   "source": [
    "## **Distance Between Vectors:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79d6ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a48a0911",
   "metadata": {},
   "source": [
    "------\n",
    "----\n",
    "-----\n",
    "------\n",
    "-----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b9ea8",
   "metadata": {},
   "source": [
    "## **Vector Projection:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a501f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99038728",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "-----\n",
    "-----\n",
    "-----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d41d22",
   "metadata": {},
   "source": [
    "## **Scalar Projection:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d8ea2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db98c9b4",
   "metadata": {},
   "source": [
    "-----\n",
    "----\n",
    "-----\n",
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079d526",
   "metadata": {},
   "source": [
    "## **Angle Between Vectors:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c194f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4239c5bd",
   "metadata": {},
   "source": [
    "----\n",
    "-----\n",
    "----\n",
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30414035",
   "metadata": {},
   "source": [
    "## **Linear Combination of Vectors:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2c055",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7add6b5",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "-----\n",
    "-----\n",
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a75e1d",
   "metadata": {},
   "source": [
    "## **Hadamard product:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa748b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29e6d65",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----\n",
    "----\n",
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f6135",
   "metadata": {},
   "source": [
    "## **Matrix-vector multiplication:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80834d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9582772e",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "----\n",
    "-----\n",
    "-----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c59a51",
   "metadata": {},
   "source": [
    "## **Vector-matrix multiplication:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0559adf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "933ee1e5",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "-----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86ae3b",
   "metadata": {},
   "source": [
    "## **Transpose of Vectors:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201e667",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
